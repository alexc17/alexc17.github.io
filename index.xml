bplist00Ñ_WebMainResourceÕ	
_WebResourceTextEncodingName^WebResourceURL_WebResourceFrameName_WebResourceData_WebResourceMIMETypeUUTF-8_fhttps://raw.githubusercontent.com/gohugoio/hugo/master/tpl/tplimpl/embedded/templates/_default/rss.xmlPOó<html><head></head><body><pre style="word-wrap: break-word; white-space: pre-wrap;">&lt;?xml version="1.0" encoding="utf-8" standalone="yes"?&gt;
&lt;rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"&gt;
  &lt;channel&gt;
    &lt;title&gt;Alexander Clark&#39;s homepage on Rational Empiricism&lt;/title&gt;
    &lt;link&gt;https://alexc17.github.io/&lt;/link&gt;
    &lt;description&gt;Recent content in Alexander Clark&#39;s homepage on Rational Empiricism&lt;/description&gt;
    &lt;generator&gt;Hugo -- gohugo.io&lt;/generator&gt;
    &lt;language&gt;en-us&lt;/language&gt;
    &lt;managingEditor&gt;alexsclark@gmail.com (Alexander Clark)&lt;/managingEditor&gt;
    &lt;webMaster&gt;alexsclark@gmail.com (Alexander Clark)&lt;/webMaster&gt;
    &lt;copyright&gt;2021 Alexander Clark All rights reserved&lt;/copyright&gt;
    &lt;lastBuildDate&gt;Thu, 09 Dec 2021 10:04:30 +0000&lt;/lastBuildDate&gt;&lt;atom:link href="https://alexc17.github.io/index.xml" rel="self" type="application/rss+xml" /&gt;
    &lt;item&gt;
      &lt;title&gt;Linguistic Nativism and the Argument from the Poverty of the Stimulus&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/pages/aps/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 07 Dec 2021 12:35:12 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/pages/aps/&lt;/guid&gt;
      &lt;description&gt;This unique contribution to the ongoing discussion of language acquisition considers the Argument from the Poverty of the Stimulus in language learning in the context of the wider debate over cognitive, computational, and linguistic issues. Critically examines the Argument from the Poverty of the Stimulus - the theory that the linguistic input which children receive is insufficient to explain the rich and rapid development of their knowledge of their first language(s) through general learning mechanisms Focuses on formal learnability properties of the class of natural languages, considered from the perspective of several learning theoretic models The only current book length study of arguments for the poverty of the stimulus which focuses on the computational learning theoretic aspects of the problem.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Empiricism and Language Learnability&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/pages/empiricism/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 07 Dec 2021 12:35:05 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/pages/empiricism/&lt;/guid&gt;
      &lt;description&gt;This interdisciplinary new work explores one of the central theoretical problems in linguistics: learnability. The authors, from different backgrounds - linguistics, philosophy, computer science, psychology and cognitive science-explore the idea that language acquisition proceeds through general purpose learning mechanisms, an approach that is broadly empiricist both methodologically and psychologically.
For many years, the empiricist approach has been taken to be unfeasible on practical and theoretical grounds. In the book, the authors present a variety of precisely specified mathematical and computational results that show that empiricist approaches can form a viable solution to the problem of language acquisition.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Handbook&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/pages/handbook/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 07 Dec 2021 12:34:59 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/pages/handbook/&lt;/guid&gt;
      &lt;description&gt;This comprehensive reference work provides an overview of the concepts, methodologies, and applications in computational linguistics and natural language processing (NLP). Features contributions by the top researchers in the field, reflecting the work that is driving the discipline forward Includes an introduction to the major theoretical issues in these fields, as well as the central engineering applications that the work has produced Presents the major developments in an accessible way, explaining the close connection between scientific understanding of the computational properties of natural language and the creation of effective language technologies Serves as an invaluable state-of-the-art reference source for computational linguists and software engineers developing NLP applications in industrial research and development labs of software companies.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Research&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/pages/research/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 07 Dec 2021 10:23:33 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/pages/research/&lt;/guid&gt;
      &lt;description&gt;I am interested in the human ability to learn and use language, particularly at the level of syntax, and I approach it using mathematical tools. Of the two, the learning problem is much harder and more constrained, and I have been developing a set of mathematical tools for learning the sorts of grammars that we think are good representations of syntax. This ends up looking like a weird sort of theoretical machine learning, but I consider it theoretical linguistics.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Bio&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/pages/bio/&lt;/link&gt;
      &lt;pubDate&gt;Mon, 06 Dec 2021 14:03:17 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/pages/bio/&lt;/guid&gt;
      &lt;description&gt;My first degree was in mathematics; I went to Trinity College, Cambridge. After a few years doing other things, I went to the University of Sussex and did a Ph.D.
I was a post-doc at the University of Geneva in ISSCO, one of the Dalle Molle research institutes, which is now a research unit called TIM.
I was a lecturer in the computer science department at Royal Holloway, and then in the philosophy department at King&amp;rsquo;s College, London.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;About Me&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/pages/aboutme/&lt;/link&gt;
      &lt;pubDate&gt;Mon, 06 Dec 2021 13:37:33 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/pages/aboutme/&lt;/guid&gt;
      &lt;description&gt;I live in London and do research on mathematical and theoretical linguistics, in particular on the problem of language acquisition. I am currently an associate researcher at CLASP at the University of Gothenburg.
I am an action editor for JMLR and TACL, two excellent open access journals. I don&amp;rsquo;t review for non open-access publications.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Upper and lower bounds&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/posts/my-first-post/&lt;/link&gt;
      &lt;pubDate&gt;Mon, 06 Dec 2021 12:04:16 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/posts/my-first-post/&lt;/guid&gt;
      &lt;description&gt;A small methodological issue. Suppose you are interested in identifying some single ingredient behind the faculty of language, some property that distinguishes humans from non-humans. This has to be some property that humans have that non-humans don&amp;rsquo;t have. So the argument needs to have two parts: a claim that humans have this property and a claim that humans don&amp;rsquo;t have this property. So this can&amp;rsquo;t be an upper bound on the human ability, but needs to be a lower bound.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Beyond {Chomsky} Normal Form: Extending Strong Learning Algorithms for {PCFGs}&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/pmlr-v153-clark21a/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2021 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/pmlr-v153-clark21a/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2021
Abstract We extend a recent consistent strong learning algorithm for a subclass of probabilistic context-free grammars in Chomsky normal form, (Clark and Fijalkow, 2020) to a much larger class of grammars by weakening the normal form constraints to allow for a richer class of derivation structures that are not necessarily binary branching, including among other possibilities unary branching trees which introduce some technical difficulties. By modifying the structural conditions appropriately we obtain an algorithm which is computationally efficient, and a consistent estimator for the class of grammars defined.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Strong Learning of Probabilistic Tree Adjoining Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2021tag/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2021 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2021tag/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark
Year 2021
Abstract The problem of learning syntactic structure is notoriously difficult, especially with mildly context-sensitive grammars. Existing approaches to learning these types of grammars are limited &amp;ndash; they are not guaranteed to learn the correct grammars or structures, only the correct derived structures whether these are trees or strings. Here we present some progress towards strong learning of these formalisms, extending a recent result on strong learning of probabilistic context-free grammars to a class of probabilistic context-free tree grammars that is weakly equivalent to Tree-Adjoining Grammars.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Strong learning of some Probabilistic Multiple Context-Free Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2021mol/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2021 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2021mol/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark
Year 2021
Abstract This paper presents an algorithm for strong learning of probabilistic multiple context free grammars from a positive sample of strings generated by the grammars. The algorithm is shown to be a consistent estimator for a class of well-nested grammars, given by explicit structural conditions on the underlying grammar, and for grammars in this class is guaranteed to converge to a grammar which is isomorphic to the original, not just one that generates the same set of strings.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Consistent Unsupervised Estimators for Anchored {PCFG}s&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2020/&lt;/link&gt;
      &lt;pubDate&gt;Wed, 01 Jan 2020 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2020/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Fijalkow, Nathana&amp;quot;el
Year 2020
Abstract Learning probabilistic context-free grammars (PCFGs) from strings is a classic problem in computational linguistics since Horning (1969). Here we present an algorithm based on distributional learning that is a consistent estimator for a large class of PCFGs that satisfy certain natural conditions including being anchored (Stratos et al., 2016). We proceed via a reparameterization of (top-down) PCFGs that we call a bottom-up weighted context-free grammar.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Computational Learning of Syntax&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2017computational/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2017 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2017computational/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2017
Abstract Learnability has traditionally been considered to be a crucial constraint on theoretical syntax; however, the issues involved have been poorly understood, partly as a result of the lack of simple learning algorithms for various types of formal grammars. Here I discuss the computational issues involved in learning hierarchically structured grammars from strings of symbols alone. The methods involved are based on an abstract notion of the derivational context of a syntactic category, which in the most elementary case of context-free grammars leads to learning algorithms based on a form of traditional distributional analysis.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/cogs12414/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2017 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/cogs12414/&lt;/guid&gt;
      &lt;description&gt;Authors Lau, Jey Han and Clark, Alexander and Lappin, Shalom
Year 2017
Abstract The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Testing Distributional Properties of Context-Free Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/icgi16/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2017 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/icgi16/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark
Year 2017
Abstract Recent algorithms for distributional learning of context-free grammars can learn all languages defined by grammars that have certain distributional properties: the finite kernel property (FKP) and the finite context property (FCP). In this paper we present some algorithms for approximately determining whether a given grammar has one of these properties. We then present the results of some experiments that indicate that with randomly generated context-free grammars in Chomsky normal form, which generate infinite languages and are derivationally sparse, nearly all grammars have the finite kernel property, whereas the finite context property is much less common.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Distributional Learning of Context-Free and Multiple Context-Free Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/topicsgi/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2016 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/topicsgi/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Yoshinaka, Ryo
Year 2016
Abstract Natural languages require grammars beyond context-free for their description. Here we extend a family of distributional learning algorithms for context-free grammars to the class of Parallel Multiple Context-Free Grammars (PMCFGs). These grammars have two additional operations beyond the simple context-free operation of concatenation: the ability to interleave strings of symbols, and the ability to copy or duplicate strings. This allows the grammars to generate some non-semilinear languages, which are outside the class of mildly context-sensitive grammars.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Distributional Learning of Some Nonlinear Tree Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/fi2016/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2016 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/fi2016/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Kanazawa, Makoto and Kobele, Gregory M. and Yoshinaka, Ryo
Year 2016
Abstract A key component of Clark and Yoshinakaâ€™s distributional learning algorithms is the extraction of substructures and contexts contained in the input data. This problem often becomes intractable with nonlinear grammar formalisms due to the fact that more than polynomially many substructures and/or contexts may be contained in each object. Previous works on distributional learning of nonlinear grammars avoided this difficulty by restricting the substructures or contexts that are made available to the learner.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Canonical Context-Free Grammars and Strong Learning: Two Approaches&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/mol2015/&lt;/link&gt;
      &lt;pubDate&gt;Thu, 01 Jan 2015 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/mol2015/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2015
Abstract Strong learning of context-free grammars is the problem of learning a grammar which is not just weakly equivalent to a target grammar but isomorphic or structurally equivalent to it. This is closely related to the problem of defining a canonical grammar for the language. The current proposal for strong learning of a small class of CFGs uses grammars whose nonterminals correspond to congruence classes of the language, in particular to a subset of those that satisfy a primality condition.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Empiricism and language learnability&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/chater2015empiricism/&lt;/link&gt;
      &lt;pubDate&gt;Thu, 01 Jan 2015 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/chater2015empiricism/&lt;/guid&gt;
      &lt;description&gt;Authors Chater, Nick and Clark, Alexander and Goldsmith, John A and Perfors, Andy
Year 2015
Abstract his interdisciplinary new work explores one of the central theoretical problems in linguistics: learnability. The authors, from different backgrounds - linguistics, philosophy, computer science, psychology and cognitive science-explore the idea that language acquisition proceeds through general purpose learning mechanisms, an approach that is broadly empiricist both methodologically and psychologically.
For many years, the empiricist approach has been taken to be unfeasible on practical and theoretical grounds.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Learnability&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2015learnability/&lt;/link&gt;
      &lt;pubDate&gt;Thu, 01 Jan 2015 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2015learnability/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2015
Abstract Reviews of learnability in linguistics focus on negative results, with the nativists stressing the negative results and the researchers of a more empiricist persuasion downplaying them. This chapter discusses the theory of learnability or grammatical inference, from a positive perspective. It focuses on the methodological issues involved in applying the tools of mathematical analysis to the empirical problem of language acquisition, and the various assumptions that one make, and by discussing the problems of grammatical inference.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;An Algebraic Approach to Multiple Context-Free Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/lacl14/&lt;/link&gt;
      &lt;pubDate&gt;Wed, 01 Jan 2014 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/lacl14/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Yoshinaka, Ryo
Year 2014
Abstract We define an algebraic structure, Paired Complete Idempotent Semirings (pcis), which are appropriate for defining a denotational semantics for multiple context-free grammars of dimension 2 (2-mcfg). We demonstrate that homomorphisms of this structure will induce well-behaved morphisms of the grammar, and generalize the syntactic concept lattice from context-free grammars to the 2-mcfg case. We show that this lattice is the unique minimal structure that will interpret the grammar faithfully and that therefore 2-mcfgs without mergeable nonterminals will have nonterminals that correspond to elements of this structure.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;An introduction to multiple context free grammars for linguists&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2014introduction/&lt;/link&gt;
      &lt;pubDate&gt;Wed, 01 Jan 2014 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2014introduction/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2014
Abstract This is a gentle introduction to Multiple Context Free Grammars (mcfgs), intended for linguists who are familiar with context free grammars and movement based analyses of displaced constituents, but unfamiliar with Minimalist Grammars or other mildly context-sensitive formalisms.
Links link
local copy of pdf
citation in bibtex&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Distributional learning of parallel multiple context-free grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clarkyoshinakamlj2013/&lt;/link&gt;
      &lt;pubDate&gt;Wed, 01 Jan 2014 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clarkyoshinakamlj2013/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Yoshinaka, Ryo
Year 2014
Abstract Natural languages require grammars beyond context-free for their description. Here we extend a family of distributional learning algorithms for context-free grammars to the class of Parallel Multiple Context-Free Grammars (PMCFGs). These grammars have two additional operations beyond the simple context-free operation of concatenation: the ability to interleave strings of symbols, and the ability to copy or duplicate strings. This allows the grammars to generate some non-semilinear languages, which are outside the class of mildly context-sensitive grammars.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Complexity in Language Acquisition&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/topicscomplexity/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 01 Jan 2013 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/topicscomplexity/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Lappin, Shalom
Year 2013
Abstract Learning theory has frequently been applied to language acquisition, but discussion has largely focused on information theoretic problemsâ€”in particular on the absence of direct negative evidence. Such arguments typically neglect the probabilistic nature of cognition and learning in general. We argue first that these arguments, and analyses based on them, suffer from a major flaw: they systematically conflate the hypothesis class and the learnable concept class.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/jmlr13/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 01 Jan 2013 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/jmlr13/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark
Year 2013
Abstract Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially infinite, set of strings generated by some target grammar. Here we define the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;{The syntactic concept lattice: Another algebraic theory of the context-free languages?}&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/scl/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 01 Jan 2013 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/scl/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2013
Abstract {The syntactic concept lattice is a residuated lattice associated with a given formal language; it arises naturally as a generalization of the syntactic monoid in the analysis of the distributional structure of the language. In this article we define the syntactic concept lattice and present its basic properties, and its relationship to the universal automaton and the syntactic congruence; we consider several different equivalent definitions, as Galois connections, as maximal factorizations and finally using universal algebra to define it as an object that has a certain universal (terminal) property in the category of complete idempotent semirings that recognize a given language, applying techniques from automata theory to the theory of context-free grammars (CFGs).&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Beyond Semilinearity: Distributional Learning of Parallel Multiple Context-free Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/pmlr-v21-clark12a/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2012 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/pmlr-v21-clark12a/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Yoshinaka, Ryo
Year 2012
Abstract Semilinearity is widely held to be a linguistic invariant but, controversially, some linguistic phenomena in languages like Old Georgian and Yoruba seem to violate this constraint. In this paper we extend distributional learning to the class of parallel multiple context-free grammars, a class which as far as is known includes all attested natural languages, even taking an extreme view on these examples.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Computational Learning Theory and Language Acquisition&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2012445/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2012 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2012445/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark and Shalom Lappin
Year 2012
Abstract no abstract
Links link
local copy of pdf
citation in bibtex&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Logical grammars, logical theories&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/lacl2012/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2012 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/lacl2012/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2012
Abstract Residuated lattices form one of the theoretical backbones of the Lambek Calculus as the standard free models. They also appear in grammatical inference as the syntactic concept lattice, an algebraic structure canonically defined for every language L based on the lattice of all distributionally definable subsets of strings. Recent results show that it is possible to build representations, such as context-free grammars, based on these lattices, and that these representations will be efficiently learnable using distributional learning.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;A Language Theoretic Approach to Syntactic Structure&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark11molx/&lt;/link&gt;
      &lt;pubDate&gt;Sat, 01 Jan 2011 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark11molx/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2011
Abstract We consider the idea of defining syntactic structure relative to a language, rather than to a grammar for a language. This allows us to define a notion of hierarchical structure that is independent of the particular grammar, and that depends rather on the properties of various algebraic structures canonically associated with a language. Our goal is not necessarily to recover the traditional ideas of syntactic structure invented by linguists, but rather to come up with an objective notion of syntactic structure that can be used for semantic interpretation.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Inference of Inversion Transduction Grammars&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/icml2011/&lt;/link&gt;
      &lt;pubDate&gt;Sat, 01 Jan 2011 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/icml2011/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark
Year 2011
Abstract We present the first polynomial algorithm for learning a class of inversion transduction grammars (ITGs) that implement context free transducers &amp;ndash; functions from strings to strings. The class of transductions that we can learn properly includes all subsequential transductions. These algorithms are based on a generalisation of distributional learning; we prove correctness of our algorithm under an identification in the limit model.
Links link&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;{Linguistic Nativism and the Poverty of the Stimulus}&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clarklappinpoverty/&lt;/link&gt;
      &lt;pubDate&gt;Sat, 01 Jan 2011 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clarklappinpoverty/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark and Shalom Lappin
Year 2011
Abstract This unique contribution to the ongoing discussion of language acquisition considers the Argument from the Poverty of the Stimulus in language learning in the context of the wider debate over cognitive, computational, and linguistic issues. Critically examines the Argument from the Poverty of the Stimulus - the theory that the linguistic input which children receive is insufficient to explain the rich and rapid development of their knowledge of their first language(s) through general learning mechanisms Focuses on formal learnability properties of the class of natural languages, considered from the perspective of several learning theoretic models The only current book length study of arguments for the poverty of the stimulus which focuses on the computational learning theoretic aspects of the problem.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Distributional Learning of some Context-free Languages with a Minimally Adequate Teacher&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark10matcfg/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2010 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark10matcfg/&lt;/guid&gt;
      &lt;description&gt;Authors Alexander Clark
Year 2010
Abstract Angluin showed that the class of regular languages could be learned from a Minimally Adequate Teacher (MAT) providing membership and equivalence queries. Clark and Eyraud (2007) showed that some context free grammars can be identified in the limit from positive data alone by identifying the congruence classes of the language. In this paper we consider learnability of context free languages using a MAT. We show that there is a natural class of context free languages, that includes the class of regular languages, that can be polynomially learned from a MAT, using an algorithm that is an extension of Angluin&amp;rsquo;s LSTAR algorithm.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Learning Context Free Grammars with the Syntactic Concept Lattice&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/icgi10scl/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2010 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/icgi10scl/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2010
Abstract The Syntactic Concept Lattice is a residuated lattice based on the distributional structure of a language; the natural representation based on this is a context sensitive formalism. Here we examine the possibility of basing a context free grammar (cfg) on the structure of this lattice; in particular by choosing non-terminals to correspond to concepts in this lattice. We present a learning algorithm for context free grammars which uses positive data and membership queries, and prove its correctness under the identification in the limit paradigm.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;The Handbook of Computational Linguistics and Natural Language Processing&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark10handbook/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2010 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark10handbook/&lt;/guid&gt;
      &lt;description&gt;Editors Alexander Clark and Chris Fox and Shalom Lappin
Year 2010
Abstract This comprehensive reference work provides an overview of the concepts, methodologies, and applications in computational linguistics and natural language processing (NLP). Features contributions by the top researchers in the field, reflecting the work that is driving the discipline forward Includes an introduction to the major theoretical issues in these fields, as well as the central engineering applications that the work has produced Presents the major developments in an accessible way, explaining the close connection between scientific understanding of the computational properties of natural language and the creation of effective language technologies Serves as an invaluable state-of-the-art reference source for computational linguists and software engineers developing NLP applications in industrial research and development labs of software companies.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Three Learnable Models for the Description of Language&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/lata2010/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2010 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/lata2010/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2010
Abstract Learnability is a vital property of formal grammars: representation classes should be defined in such a way that they are learnable. One way to build learnable representations is by making them objective or empiricist: the structure of the representation should be based on the structure of the language. Rather than defining a function from representation to language we should start by defining a function from the language to the representation: following this strategy gives classes of representations that are easy to learn.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Unsupervised learning and grammar induction&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2010unsupervised/&lt;/link&gt;
      &lt;pubDate&gt;Fri, 01 Jan 2010 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2010unsupervised/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Lappin, Shalom
Year 2010
Abstract In this chapter we consider unsupervised learning from two perspectives. First, we briefly look at its advantages and disadvantages as an engineering technique applied to large corpora in natural language processing. While supervised learning generally achieves greater accuracy with less data, unsupervised learning offers significant savings in the intensive labor required for annotating text. Second, we discuss the possible relevance of unsupervised learning to debates on the cognitive basis of human language acquisition.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Another Look at Indirect Negative Evidence&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark-lappin-2009-another/&lt;/link&gt;
      &lt;pubDate&gt;Thu, 01 Jan 2009 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark-lappin-2009-another/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Lappin, Shalom
Year 2009
Abstract Indirect negative evidence is clearly an important way for learners to constrain overgeneralisation, and yet a good learning theoretic analysis has yet to be provided for this, whether in a PAC or a probabilistic identification in the limit framework. In this paper we suggest a theoretical analysis of indirect negative evidence that allows the presence of ungrammatical strings in the input and also accounts for the relationship between grammaticality/acceptability and probability.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;A Polynomial Algorithm for the Inference of Context Free Languages&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/icgi2008bfg/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 01 Jan 2008 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/icgi2008bfg/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Eyraud, R{&#39;e}mi and Habrard, Amaury
Year 2008
Abstract We present a polynomial algorithm for the inductive inference of a large class of context free languages, that includes all regular languages. The algorithm uses a representation which we call Binary Feature Grammars based on a set of features, capable of representing richly structured context free languages as well as some context sensitive languages. More precisely, we focus on a particular case of this representation where the features correspond to contexts appearing in the language.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Polynomial Identification in the Limit of Substitutable Context-free Languages&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark07ax/&lt;/link&gt;
      &lt;pubDate&gt;Mon, 01 Jan 2007 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark07ax/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Eyraud, R&#39;emi
Year 2007
Abstract This paper formalises the idea of substitutability introduced by Zellig Harris in the 1950s and makes it the basis for a learning algorithm from positive data only for a subclass of context-free languages. We show that there is a polynomial characteristic set, and thus prove polynomial identification in the limit of this class. We discuss the relationship of this class of languages to other common classes discussed in grammatical inference.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;PAC-Learning Unambiguous NTS Languages&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/icgi2006pac/&lt;/link&gt;
      &lt;pubDate&gt;Sun, 01 Jan 2006 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/icgi2006pac/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2006
Abstract Non-terminally separated (NTS) languages are a subclass of deterministic context free languages where there is a stable relationship between the substrings of the language and the non-terminals of the grammar. We show that when the distribution of samples is generated by a PCFG, based on the same grammar as the target language, the class of unambiguous NTS languages is PAC-learnable from positive data alone, with polynomial bounds on data and computation.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Partially Distribution-Free Learning of Regular Languages from Positive Samples&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark04b/&lt;/link&gt;
      &lt;pubDate&gt;Thu, 01 Jan 2004 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark04b/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Thollard, Franck
Year 2004
Abstract Regular languages are widely used in NLP today in spite of their shortcomings. Efficient algorithms that can reliably learn these languages, and which must in realistic applications only use positive samples, are necessary. These languages are not learnable under traditional distribution free criteria. We claim that an appropriate learning framework is PAC learning where the distributions are constrained to be generated by a class of stochastic automata with support equal to the target concept.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;{PAC}-learnability of Probabilistic Deterministic Finite State Automata&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark04a/&lt;/link&gt;
      &lt;pubDate&gt;Thu, 01 Jan 2004 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark04a/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander and Thollard, Franck
Year 2004
Abstract We study the learnability of Probabilistic Deterministic Finite State Automata under a modified PAC-learning criterion. We argue that it is necessary to add additional parameters to the samplecomplexity polynomial, namely a bound on the expected length of strings generated from any state,and a bound on the distinguishability between states. With this, we demonstrate that the class of PDFAs is PAC-learnable using a variant of a standard state-merging algorithm and the Kullback-Leibler divergence as error function.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Combining Distributional and Morphological Information for Part of Speech Induction&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark-2003-combining/&lt;/link&gt;
      &lt;pubDate&gt;Wed, 01 Jan 2003 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark-2003-combining/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2003
Abstract In this paper we discuss algorithms for clustering words into classes from unlabelled text using unsupervised algorithms, based on distributional and morphological information. We show how the use of morphological information can improve the performance on rare words, and that this is robust across a wide range of languages.
Links link
local copy of pdf
citation in bibtex&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Pre-processing very noisy text&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark2003pre/&lt;/link&gt;
      &lt;pubDate&gt;Wed, 01 Jan 2003 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark2003pre/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2003
Abstract no abstract
Links local copy of pdf
citation in bibtex&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Shallow Parsing Using Probabilistic Grammatical Inference&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/icgi2002/&lt;/link&gt;
      &lt;pubDate&gt;Tue, 01 Jan 2002 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/icgi2002/&lt;/guid&gt;
      &lt;description&gt;Authors Thollard, Franck and Clark, Alexander
Year 2002
Abstract This paper presents an application of grammatical inference to the task of shallow parsing. We first learn a deterministic probabilistic automaton that models the joint distribution of Chunk (syntactic phrase) tags and Part-of-speech tags, and then use this automaton as a transducer to find the most likely chunk tag sequence using a dynamic programming algorithm. We discuss an efficient means of incorporating lexical information, which automatically identifies particular words that are useful using a mutual information criterion, together with an application of bagging that improve our results.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Unsupervised Induction of Stochastic Context Free Grammars with Distributional Clustering&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark-01a/&lt;/link&gt;
      &lt;pubDate&gt;Mon, 01 Jan 2001 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark-01a/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2001
Abstract An algorithm is presented for learning a phrase-structure grammar from tagged text. It clusters sequences of tags together based on local distributional information, and selects clusters that satisfy a novel mutual information criterion. This criterion is shown to be related to the entropy of a random variable associated with the tree structures, and it is demonstrated that it selects linguistically plausible constituents. This is incorporated in a Minimum Description Length algorithm.&lt;/description&gt;
    &lt;/item&gt;
    
    &lt;item&gt;
      &lt;title&gt;Inducing Syntactic Categories by Context Distribution Clustering&lt;/title&gt;
      &lt;link&gt;https://alexc17.github.io/publication/clark-2000-inducing/&lt;/link&gt;
      &lt;pubDate&gt;Sat, 01 Jan 2000 00:00:00 +0000&lt;/pubDate&gt;
      &lt;author&gt;alexsclark@gmail.com (Alexander Clark)&lt;/author&gt;
      &lt;guid&gt;https://alexc17.github.io/publication/clark-2000-inducing/&lt;/guid&gt;
      &lt;description&gt;Authors Clark, Alexander
Year 2000
Abstract This paper addresses the issue of the automatic induction of syntactic categories from unanno- tared corpora. Previous techniques give good results, but fail to cope well with ambiguity or rare words. An algorithm, context distribution clustering (CDC), is presented which can be naturally extended to handle these problems.
Links link
local copy of pdf
citation in bibtex&lt;/description&gt;
    &lt;/item&gt;
    
  &lt;/channel&gt;
&lt;/rss&gt;
</pre></body></html>Ztext/plain    ( F U l ~ ” š	û                           
