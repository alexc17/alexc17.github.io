%!PS-Adobe-3.0%%Title: (Microsoft Word - SprolacPaper.doc)%%Creator: (Microsoft Word: LaserWriter 8 Z1-8.7.1)%%CreationDate: (10:01 AM Friday, March 21, 2003)%%For: (Clare Hornsby)%%Pages: 10%%DocumentFonts: TimesNewRomanPSMT TimesNewRomanPS-BoldMT Arial-BoldMT Symbol ArialMT TimesNewRomanPS-ItalicMT%%DocumentNeededFonts: TimesNewRomanPSMT TimesNewRomanPS-BoldMT Arial-BoldMT Symbol ArialMT TimesNewRomanPS-ItalicMT%%DocumentSuppliedFonts: %%DocumentData: Clean7Bit%%PageOrder: Ascend%%Orientation: Portrait%%DocumentMedia: (Default) 595.2 841.92 0 () ()%RBINumCopies: 1%RBINupNess: 1 1%ADO_ImageableArea: 28.08 30 566.16 810.96%RBIDocumentSuppliedFonts: %%EndComments%%BeginDefaults%%ViewingOrientation: 1 0 0 1%%EndDefaultsuserdict/dscInfo 5 dict dup begin/Title(Microsoft Word - SprolacPaper.doc)def/Creator(Microsoft Word: LaserWriter 8 Z1-8.7.1)def/CreationDate(10:01 AM Friday, March 21, 2003)def/For(Clare Hornsby)def/Pages 10 defend put%%BeginProlog/md 220 dict def md begin/currentpacking where {pop /sc_oldpacking currentpacking def true setpacking}if%%BeginFile: lw8_feature-1.01%%Copyright: Copyright 1990-1999 Adobe Systems Incorporated and Apple Computer Incorporated. All Rights Reserved./bd{bind def}bind def/ld{load def}bd/xs{exch store}bd/Z{0 def}bd/T true def/F false def/level2/languagelevel where{pop languagelevel 2 ge}{F}ifelsedef/odictstk Z/oopstk Z/fcl{count oopstk sub dup 0 gt{{pop}repeat}{pop}ifelsecountdictstack odictstk sub dup 0 gt{{end}repeat}{pop}ifelse}bd/sfcl2{/odictstk countdictstack storecount/oopstk xs}bd/efcl2{stopped{$error/newerror F put}iffcl}bd/noload Z/startnoload{{/noload save store}if}bd/endnoload{{noload restore}if}bd/setcopies{level2{1 dict begin/NumCopies exch def currentdict end setpagedevice}{userdict/#copies 3 -1 roll put}ifelse}deflevel2 startnoload/ststpgdev{}def/dopgdev{}def/stpgdev{}def/buf Z/didstop T def/sfcl{/didstop T store/odictstk countdictstack storecount/oopstk xscurrentfile cvx stopped{$error/newerror F putdidstop{save/didstop xs/buf vmstatus exch sub exch pop dup 0 lt{pop 0}ifdup 64000 gt{pop 64000}if string store{currentfile buf readline{(}efcl)eq{exit}if}{/UnexpectedEOF errordict/rangecheck get exec}ifelse}loopdidstop restore}if}iffcl}bd/efcl{/didstop F storeexecstop}bdlevel2 endnoload level2 not startnoload/setpagedevice where{pop/realstpgdev/setpagedevice ld}if/SC_topddict Z/SC_spdict Z/$spusrdict F def/dopgdev{userdict/setpagedevice undef$spusrdict{userdict/setpagedevice/realstpgdev load put/$spusrdict F store}ifSC_topddict realstpgdev}bd/stpgdev{SC_topddict dup 3 -1 roll{SC_spdict 2 index known{SC_spdict 2 index getdup 3 -1 roll{put dup}forallpop put dup}{put dup}ifelse}forallpop pop}bd/ststpgdev{/setpagedevice where{userdict eq{/$spusrdict T store}if}ifuserdict/setpagedevice/stpgdev load put/SC_topddict 0 dict store/SC_spdict 3 dict begin/InputAttributes 0 dict def/Policies 0 dict def/OutputAttributes 0 dict defcurrentdictendstore}def/sfcl/sfcl2 ld/efcl/efcl2 ldlevel2 not endnoload%%EndFile%%BeginFile: lw8_basic-4.0/xdf{exch def}bd/:L/lineto/lw/setlinewidth/:M/moveto/rl/rlineto/rm/rmoveto/:C/curveto/:T/translate/:K/closepath/:mf/makefont/gS/gsave/gR/grestore/np/newpath12{ld}repeat/framewidth -1 def/QDframwid -1 def/numframes Z/mTS matrix def/$m matrix def/av 87 def/por T def/normland F def/psb-nosave{}def/pse-nosave{}def/us Z/psb{/us save store}bd/pse{us restore}bd/level3/languagelevel where{pop languagelevel 3 ge}{F}ifelsedeflevel2 startnoload/setjob{statusdict/jobname 3 -1 roll put}bd/devg/DeviceGray def/devr/DeviceRGB def/devc/DeviceCMYK deflevel2 endnoload level2 not startnoload/setjob{1 dict begin/JobName xdf currentdict end setuserparams}bd/devg[/DeviceGray]def/devr[/DeviceRGB]def/devc[/DeviceCMYK]deflevel2 not endnoload/pm Z/mT Z/sD Z/mTSsetup{mT $m currentmatrix mTS concatmatrix pop}bd/pmSVsetup{/pm save store}bd/initializepage{mT concat}bd/endp{pm restore}bd/adjRect{dup 2 mul 6 2 roll4 index sub exch 5 -1 roll sub exch4 2 roll4 index add exch 5 -1 roll add exch4 2 roll}bd/frame1up{gSmTS setmatrixQDframwid lw/setstrokeadjust where{pop T setstrokeadjust}ifclippath pathbbox2 index sub exch3 index sub exchcurrentlinewidth framewidth muladjRectnumframes dup 0 lt{pop 0}if{4 copyrScurrentlinewidth framewidthmul 4 muladjRect}repeatpop pop pop popgR}bd/$c devr def/rectclip where{pop/rC/rectclip ld}{/rC{np 4 2 roll:M1 index 0 rl0 exch rlneg 0 rl:Kclip np}bd}ifelse/rectfill where{pop/rF/rectfill ld}{/rF{gSnp4 2 roll:M1 index 0 rl0 exch rlneg 0 rlfillgR}bd}ifelse/rectstroke where{pop/rS/rectstroke ld}{/rS{gSnp4 2 roll:M1 index 0 rl0 exch rlneg 0 rl:KstrokegR}bd}ifelse%%EndFilelevel3 startnoload%%BeginFile: lw8_safeclipL12-1.0/rectclip where{pop/rCa/rectclip ld}{/rCa{np 01 index length 4 idiv{2 copy4 getintervalaload pop4 2 roll:M1 index 0 rl0 exch rlneg 0 rl:K4 add}repeatclip nppop pop}bd}ifelse/savedstack Z/subsavedstack Z/execstring Z/saferCa{/execstring xs/odictstk countdictstack store/oopstk 0 storecount 0 ne{savedstack 0 eq{count 100 gt{count}{100}ifelsearray/savedstack xs}{count savedstack length gt{count array/savedstack xs}if}ifelsecount savedstack 0 3 -1 rollgetintervalastore/subsavedstack xs}{/subsavedstack 0 store}ifelseexecstring cvx stopped{$error/newerror F putnewpath}iffclsubsavedstack 0 ne{subsavedstack aload pop}if}bd%%EndFilelevel3 endnoload level3 not startnoload%%BeginFile: lw8_safeclipL3-1.0/saferCa/pop ld%%EndFilelevel3 not endnoload%%BeginFile: lw8_level1_colorspace-2.0/G/setgray ld/:F1/setgray ld/:F/setrgbcolor ld/:F4/setcmykcolor where{pop/setcmykcolor ld}{{3{dup3 -1 roll adddup 1 gt{pop 1}if1 exch sub4 1 roll}repeatpopsetrgbcolor}bd}ifelse/:Fx{counttomark{0{G}0{:F}{:F4}}exch getexecpop}bd/$cs Z/:rg{devr :ss}bd/:sc{$cs :ss}bd/:dc{dup type/arraytype eq{0 get}ifdup/DeviceCMYK eq{pop devc}{/DeviceGray eq{devg}{devr}ifelse}ifelse/$cs xdf}bd/:sgl{}def/:dr{}bd/:fCRD{pop}bd/:ckcs{}bd/:ss{/$c xdf}bd%%EndFile%%BeginFile: lw8_uniform_graphics-2.0/@a{np :M 0 rl :L 0 exch rl 0 rl :L fill}bd/@b{np :M 0 rl 0 exch rl :L 0 rl 0 exch rl fill}bd/@c{moveto 0 rlineto stroke}bd/@w{moveto 0 exch rlineto stroke}bd/arct where{pop}{/arct{arcto pop pop pop pop}bd}ifelse/x1 Z/x2 Z/y1 Z/y2 Z/rad Z/@q{/rad xs/y2 xs/x2 xs/y1 xs/x1 xsnpx2 x1 add 2 div y1 :Mx2 y1 x2 y2 rad arctx2 y2 x1 y2 rad arctx1 y2 x1 y1 rad arctx1 y1 x2 y1 rad arctfill}bd/@s{/rad xs/y2 xs/x2 xs/y1 xs/x1 xsnpx2 x1 add 2 div y1 :Mx2 y1 x2 y2 rad arctx2 y2 x1 y2 rad arctx1 y2 x1 y1 rad arctx1 y1 x2 y1 rad arct:Kstroke}bd/@i{np 0 360 arc fill}bd/@j{gSnp:Tscale0 0 .5 0 360 arcfillgR}bd/@e{np0 360 arc:Kstroke}bd/@f{np$m currentmatrixpop:Tscale0 0 .5 0 360 arc:K$m setmatrixstroke}bd/@k{gSnp:T0 0 :M0 0 5 2 rollarc fillgR}bd/@l{gSnp:T0 0 :Mscale0 0 .5 5 -2 roll arcfillgR}bd/@m{nparcstroke}bd/@n{np$m currentmatrixpop:Tscale0 0 .5 5 -2 roll arc$m setmatrixstroke}bd%%EndFile%%BeginFile: lw8_basic_text-3.0/S/show ld/A{0.0 exch ashow}bd/R{0.0 exch 32 exch widthshow}bd/W{0.0 3 1 roll widthshow}bd/J{0.0 32 4 2 roll 0.0 exch awidthshow}bd/V{0.0 4 1 roll 0.0 exch awidthshow}bd/fcflg T def/fc{fcflg{vmstatus exch sub 50000 lt{(%%[ Warning: Running out of memory ]%%\r)print flush/fcflg F store}if pop}if}bd/$f[1 0 0 -1 0 0]def/:ff{$f :mf}bd/$o 1. def/gl{$o G}bd/MacEncoding StandardEncoding 256 array copy defMacEncoding dup 9/space put dup 39/quotesingle put 96/grave put/Adieresis/Aring/Ccedilla/Eacute/Ntilde/Odieresis/Udieresis/aacute/agrave/acircumflex/adieresis/atilde/aring/ccedilla/eacute/egrave/ecircumflex/edieresis/iacute/igrave/icircumflex/idieresis/ntilde/oacute/ograve/ocircumflex/odieresis/otilde/uacute/ugrave/ucircumflex/udieresis/dagger/degree/cent/sterling/section/bullet/paragraph/germandbls/registered/copyright/trademark/acute/dieresis/notequal/AE/Oslash/infinity/plusminus/lessequal/greaterequal/yen/mu/partialdiff/summation/product/pi/integral/ordfeminine/ordmasculine/Omega/ae/oslash/questiondown/exclamdown/logicalnot/radical/florin/approxequal/Delta/guillemotleft/guillemotright/ellipsis/space/Agrave/Atilde/Otilde/OE/oe/endash/emdash/quotedblleft/quotedblright/quoteleft/quoteright/divide/lozenge/ydieresis/Ydieresis/fraction/Euro/guilsinglleft/guilsinglright/fi/fl/daggerdbl/periodcentered/quotesinglbase/quotedblbase/perthousand/Acircumflex/Ecircumflex/Aacute/Edieresis/Egrave/Iacute/Icircumflex/Idieresis/Igrave/Oacute/Ocircumflex/apple/Ograve/Uacute/Ucircumflex/Ugrave/dotlessi/circumflex/tilde/macron/breve/dotaccent/ring/cedilla/hungarumlaut/ogonek/caronMacEncoding 128 128 getinterval astore poplevel2 startnoload/copyfontdict{findfont dup length dictbegin{1 index/FID ne{def}{pop pop}ifelse}forall}bd/$ckeyd md def/:skey{1 index maxlength 2 indexlength subge{begin/$mkeys 20 dict def/$mkeys loadenddup/$ckeyd xs}if3 1 roll put}bd/fD1pass{40$ckeyd//mdne{pop 1}if$ckeyd exch:skey}bd/:searchdict Z/:searchdict{exch 2 copyknown{get}{exch/$mkeysget:searchdict}ifelse}bd/lU{//md exch 2 copyknown{get}{exch/$mkeys get:searchdict}ifelse}bd/:xs{exch 0 1 2 index length 1 sub{3 copygS 1 getinterval show gRexchget 0.0rm}for pop pop}bd/:ys{exch 0 1 2 index length 1 sub{3 copygS 1 getinterval show gRexchget 0.0 exchrm}for pop pop}bd/:xys{exch 0 1 2 index length 1 sub{3 copygS 1 getinterval show gRexch 2 mul 2 copy1 add get 3 -2 roll get exchrm}for pop pop}bdlevel2 endnoload level2 not startnoload/copyfontdict{findfont dup length dictcopybegin}bd/fD1pass/def ld/lU/load ldlevel2{/:xs/xshow ld/:ys/yshow ld/:xys/xyshow ld}iflevel2 not endnoload/:xbl{currentpoint2 index exec:M.03 ps mul currentpoint2 index 0.0rm3 indexexec:M currentpoint2 index duprm3 index exec:M0.0 1 indexrm exchexecneg 0.0 exchrm}bd/:xot{currentpoint -2.0 0.0 rm 2 index exec :Mcurrentpoint 0.0 -2.0 rm 2 index exec :Mcurrentpoint 0.0 2.0 rm 2 index exec :Mcurrentpoint 2.0 0.0 rm 2 index exec :MgS gl exec currentpoint gR :M}bd/:xsh{2 ps 0.05 mul add2 copycurrentpoint5 2 roll0.0 rmexeccurrentpoint6 2 rollsub moveto:xotmoveto}bd/fD Z/sf Z/scf Z/sf1pass{lU setfont}bd/scf1pass{exch lU exch scalefont fD}bd/scf2pass{scalefont fD}bdmd/fontname known not{/fontname/customfont def}if/Encoding Z/:mre{copyfontdict/Encoding MacEncoding deffontname currentdictenddefinefont :ff fD}bd/:bsr{copyfontdict/Encoding Encoding 256 array copy defEncoding dup}bd/pd{put dup}bd/:esr{pop popfontname currentdictenddefinefont :ff fD}bd/ps Z/fz{/ps xs}bd/cF/currentfont ld/mbf{/makeblendedfont where{popmakeblendedfont/ABlend exch definefont}{pop}ifelsefD}def%%EndFile/currentpacking where {pop sc_oldpacking setpacking}if end%%EndProlog%%BeginSetupmd begin/fD/def ld/sf/setfont ld /scf/scf2pass ld%RBIIncludeNonPPDFeature: NumCopies 1%RBIBeginNonPPDFeature: WaitTimeout 600 600/languagelevel where{pop languagelevel 2 ge}{false}ifelse{1 dict dup/WaitTimeout 4 -1 roll put setuserparams}{statusdict/waittimeout 3 -1 roll put}ifelse%RBIEndNonPPDFeaturesfcl{%%BeginFeature: *InputSlot Upper%%EndFeature
}efcl
sfcl{%%BeginFeature: *ManualFeed Falselevel2 {1 dict dup /ManualFeed false put setpagedevice}{statusdict begin /manualfeed false store end} ifelse%%EndFeature
}efcl
sfcl{%%BeginFeature: *PageRegion A4Smalllevel2 { 		2 dict dup /PageSize [595 842] put dup /ImagingBBox [28 30 566 811] put setpagedevice	}{		a4small	} ifelse%%EndFeature
}efcl
(Clare Hornsby)setjob/mT[.24 0 0 -.24 28.081 810.979]def%RBIIncludeStartNup/sD 16 dict def%%IncludeFont: TimesNewRomanPSMT/f47/TimesNewRomanPSMTfindfont :ff fD/f65 f47 41 scf/f78 f47 50 scf%%IncludeFont: TimesNewRomanPS-BoldMT/f91/TimesNewRomanPS-BoldMTfindfont :ff fD/f111 f91 41 scf%%IncludeFont: Arial-BoldMT/f132/Arial-BoldMTfindfont :ff fD/f147 f132 41 scf%%IncludeFont: Symbol/f169/Symbol%%BeginFile: lw8_euroSpecial-1.01/nEro/Symbol findfontbeginFontTypedup dup dup dup1 eq exch 5 eq or exch 42 eq or exch 43 eq or exch 3 eq or{currentdict/CharStrings known{CharStrings/Euro known}{true}ifelse}{true}ifelseenddefnEro startnoload10 dict begin/FontInfo 2 dict dup begin/version(001.000)def/Notice(Copyright \251 1998 Apple Computer Inc.)defend def/FontName/Europatch def/Encoding StandardEncoding def/PaintType 0 def/FontType 1 def/FontMatrix[0.001 0 0 0.001 0 0]def/FontBBox{21 -9 714 689}defcurrentdict enddup/Private 15 dict dup begin/|-{def}def/|{put}def/BlueValues[-19. 0. 487. 500. 673. 688.]|-/BlueScale 0.0526315789 def/MinFeature{16 16}|-/StdHW[92.]def/StdVW[85.]def/StemSnapH[92.]def/StemSnapV[85.]def/ForceBoldThreshold .5 def/ForceBold false def/password 5839 def/Subrs 16 arraydup 0<118cade7978c9a8ab47e7be71fa277>|dup 1<118cade79273658a5c>|dup 2<118cade7927297416d>|dup 3<118cade712>|dup 4<118cade795e45b7819d5b190>|dup 5<118cade712>|dup 6<118cade712>|dup 7<118cade79266e29ec4a224>|dup 8<118cade7926513197e6246425e>|dup 9<118cade792645d0ab32061e2268dfb>|dup 10<118cade792638e135e25d183266bd7f81e>|dup 11<118cade7927439b1>|dup 12<118cade7e644d1e7a50cacbc>|dup 13<118cade78f9ed1e3fe>|dup 14<118cade7e0d1ca3c54>|dup 15<118cade78f9edf3959>||-2 index/CharStrings 2 dict dup begin/A<118cade7b98bc82571af5aee01f90103a394bff91b0ba5c07ffa5d64ff811d8a387b6ec3142e3c549269606becee2076d12186aced6d3558a7713c6635c038cf4bf8afc6076160e8ead2af8859f19c117df2af5a56fd0c316f31ba13c15c7ce3110f9d01081b9aeb32fbe8a3618047f1e92e6e08818a4bb109a567da3f88883d9eb237a4257a9535d72a66345d6a36508b96c2805a310781de324fe691942dd7947ac02673d33943c06ae133ef93a7292b6dab>|-/.notdef<118cade79205cabfe7>|-end endput putdup/FontName get exch definefont popnEro endnoload/subfontdict Z/subfontcharsize Zfindfont duplength 2 add dictbegin{1 index/FID ne 2 index/UniqueID ne and{def}{pop pop}ifelse}forallnEro not{/subfontdict[/Europatch findfont FontMatrixmatrix invertmatrix makefontdup dup length 2 add dictbegin{1 index/FID ne 2 index/UniqueID ne and{def}{pop pop}ifelse}forall/PaintType 2 def/StrokeWidth 12 def/customfont currentdictenddefinefont]defgsaveinitgraphics/subfontcharsize[subfontdict 0 get setfont(A)stringwidth0 0 moveto(A)true charpathpathbbox]defgrestore/CharStrings CharStringsdup length 1 add dictlevel2{copy}{begin{def}forallcurrentdictend}ifelsedup/Euro{subfontcharsize aload popsetcachedevicepopsubfontdict currentdict/PaintType getdup 0 ne{pop 1}ifget setfont0 0 moveto(A)show}bind putdef}if/Encoding Encoding 256 array copydup 240/apple pd160/Euro putdeffontname/customfont eq{/Symbol}{fontname}ifelsecurrentdictenddefinefont :ff fD%%EndFile/f181 f169 41 scf%%IncludeFont: ArialMT/f194/ArialMTfindfont :ff fD/f207 f194 41 scf%%IncludeFont: TimesNewRomanPS-ItalicMT/f220/TimesNewRomanPS-ItalicMTfindfont :ff fD/f241 f220 41 scf{/Courier findfont[10 0 0 -10 0 0]:mf setfont}stopped{$error/newerror F put}if%%EndSetup%%Page: 1 1%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 1 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(1)SgRgS 0 0 2242 3254 rC819 217 :Mf78 sf(Pre-processing very noisy text)S957 275 :M(Alexander Clark)S987 332 :M(ISSCO / TIM)S908 390 :M(University of Geneva)S725 447 :M(UNI-MAIL, Boulevard du Pont-)S1375 447 :M(d'Arve,)S922 505 :M(CH-1211 Geneva 4,)S1003 562 :M(Switzerland)S848 620 :M(Alex.Clark@issco.unige.ch)S257 717 :Mf65 sf(Existing techniques for tokenisation and sentence boundary identification are extremely accurate when)S257 765 :M(the data is perfectly clean \(Mikheev, 2002\), and have been applied successfully to corpora of news)S257 813 :M(feeds and other post-edited corpora. Informal written texts are readily available, and with the growth of)S257 861 :M(other informal text modalities \(IRC, ICQ, SMS etc.\) are becoming an interesting alternative, perhaps)S257 908 :M(better suited as a source for lexical resources and language models for studies of dialogue and)S257 956 :M(spontaneous speech. However, the high degree of spelling errors, irregularities and idiosyncrasies in the)S257 1004 :M(use of punctuation, white space and capitalisation require specialised tools. In this paper we study the)S257 1052 :M(design and implementation of a tool for pre-processing and normalisation of noisy corpora. We argue)S257 1100 :M(that rather than having separate tools for tokenisation, segmentation and spelling correction organised)S257 1148 :M(in a pipeline, a unified tool is appropriate because of certain specific sorts of errors. We describe how a)S257 1196 :M(noisy channel model can be used at the character level to perform this. We describe how the sequence)S257 1244 :M(of tokens needs to be divided into various types depending on their characteristics, and also how the)S257 1292 :M(modelling of white-space needs to be conditioned on the type of the preceding and following tokens.)S257 1340 :M(We use trainable stochastic transducers to model typographical errors, and other orthographic changes)S257 1388 :M(and a variety of sequence models for white space and the different sorts of tokens. We discuss the)S257 1435 :M(training of the models and various efficiency issues related to the decoding algorithm, and illustrate this)S257 1483 :M(with examples from a 100 million word corpus of Usenet news.)S257 1579 :Mf111 sf(1.)Sf147 sf( )S332 1579 :Mf111 sf(Introduction)S257 1675 :Mf65 sf(This paper addresses the problem of the pre-processing of noisy text. We concern ourselves just with)S257 1723 :M(English text, though we note that the same approach could be applied to many other languages. All)S257 1771 :M(texts are noisy to a greater or lesser extent, and even when post-edited will contain varying amounts of)S257 1819 :M(typographical errors. A useful assumption when processing such texts is that the errors are isolated and)S257 1867 :M(surrounded by clean context that can be used to correct the errors.  Here we are interested in a more)S257 1915 :M(difficult problem -- processing texts where a high proportion of the words may have errors, and where)S257 1963 :M(indeed it may even be difficult to perform the tokenisation to identify the words.  Our goal is to)S257 2010 :M(produce a tool which can process arbitrarily noisy text and even text which has been deliberately)S257 2058 :M(obfuscated for one reason or another - for example to avoid detection of  discussions of an obscene or)S257 2106 :M(criminal nature.  Here we discuss the design and implementation of such a tool, but we shall not present)S257 2154 :M(any extensive formal evaluations of this, partly due to current difficulties with obtaining suitable)S257 2202 :M(annotated data for testing. Our basic methodology which we shall describe in detail below is the)S257 2250 :M(standard noisy channel model employed widely in the fields of speech recognition \(Jelinek 1997\) and)S257 2298 :M(other areas of Natural Language Processing such as machine translation and spelling correction)S257 2346 :M(\(Kernighan, Church and Gale 1990\) an area quite close to our own.  In brief, we consider that the text)S257 2394 :M(we study is being produced by an underlying stochastic process \321 a Language Model, and that this)S257 2442 :M(data is then put through a noisy channel, a process that converts the sequence of symbols \(abstract)S257 2490 :M(representations of words or word classes\) produced by the model into a sequence of characters. In)S257 2537 :M(speech recognition the noisy channel is the acoustic model mapping the fixed set of words in the)S257 2585 :M(vocabulary of the system to a sequence of phones and then to acoustic features.)S257 2681 :M(Here we have a radically different process: first, we must allow the system to have an unbounded)S257 2729 :M(vocabulary, and secondly the process by which the words are mapped to a sequence of characters is)S257 2777 :M(very different and needs to be decomposed carefully into a sequence of different levels, incorporating)S257 2825 :M(at the minimum the introduction of varying amounts of white space between the tokens, remembering)S257 2873 :M(of course that some tokens are written without intervening white space, various changes to the)S257 2921 :M(capitalisation of the words, including both the obligatory addition of capitalisation at the beginning of)S257 2969 :M(sentences and the optional use of capitalisation for emphasis and the removal of trailing periods from)S257 3017 :M(abbreviation occurring immediately before a period. In addition we have various types of typographical)S257 3065 :M(errors caused either by ignorance of the correct spelling or true typing errors due to lack of manual)Sendpshowpage%%PageTrailer%%Page: 2 2%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 2 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(2)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(dexterity on the keyboard. We note that there are other ways of producing electronic text which include)S257 257 :M(handwriting recognition systems on Personal Digital Assistants \(PDAs\), the output from speech)S257 305 :M(recognition systems, and the use of OCR on printed documents. These will all have different patterns of)S257 353 :M(error that we suspect could be handled using the system we present here with appropriate)S257 401 :M(modifications. We do not however have any suitable sources of data to investigate this further at the)S257 448 :M(moment.)S257 544 :Mf111 sf(2.)Sf147 sf( )S332 544 :Mf111 sf(Motivation)S257 640 :Mf65 sf(Studies involving very large corpora have often been based on the use of news feeds for obvious)S257 688 :M(technical reasons \321 exceptions are the Brown Corpus, the British National Corpus \(BNC\) and the)S257 736 :M(forthcoming American National Corpus. News feed based corpora have advantages \320 they tend to be)S257 784 :M(relatively homogeneous in register and in orthographic style \(by which we mean the use of)S257 832 :M(capitalisation, punctuation and white space\), and this facilitates their processing by potential end users.)S257 880 :M(Moreover, news feeds are an economically important source of information.  Non-news feed sources of)S257 928 :M(corpora have tended to come from several sources: one is of course transcriptions of naturally)S257 976 :M(occurring speech, such as is represented in the BNC to a certain extent, or alternatively other sources of)S257 1023 :M(highly post-edited written text such as fiction. These corpora have their own problems \320 norms of)S257 1071 :M(informal spoken language tend to change quite rapidly, and fiction tends to have stylistically marked)S257 1119 :M(language that is quite uncommon in other registers. Informal written language is becoming more and)S257 1167 :M(more widespread. Usenet, a distributed news service that  is  still a thriving forum of often acrimonious)S257 1215 :M(debate, and other forms of electronic communication with text are growing in popularity, whether on)S257 1263 :M(the Internet \(IRQ, ICQ\) or on mobile phones \(SMS\). The study of this language is intrinsically)S257 1311 :M(interesting and deserves study itself.  Additionally, it can be a very useful source for other studies of)S257 1359 :M(language. Our particular interest is in extracting adjacency pairs for dialogue processing using quoted)S257 1407 :M(sections from Usenet posts. We shall provide some illustrative examples from a corpus of 100 million)S257 1455 :M(words of Usenet posts that we have prepared over the past two years.)S257 1550 :Mf111 sf(3.)Sf147 sf( )S332 1550 :Mf111 sf(Pre-processing)S257 1646 :Mf65 sf(It is now time to make precise the set of tasks that we wish to perform. We conceive of the processing)S257 1694 :M(of texts as a sequence of operations starting from some sort of computer file or stream, and ending with)S257 1742 :M(some more detailed and structured representation. We consider the very first step -- the parsing of the)S257 1790 :M(data format, be it raw text in some character encoding, or the parsing of some structured text format)S257 1838 :M(such as XML or the native file format of a commercial word processor to be done as a preliminary step)S257 1886 :M(by other components. The stages that we are concerned with in this paper take as input the sequence of)S257 1934 :M(characters \(we use Unicode throughout\) including white space characters and so on. We wish to)S257 1982 :M(produce from this a sequence of tokens together with certain simple analyses. In particular we wish to)S257 2030 :M(identify)S332 2078 :Mf181 sf<B7>Sf207 sf( )S407 2078 :Mf65 sf(The boundaries of the tokens: that is to say the sequences of characters that form each token)S332 2125 :Mf181 sf<B7>Sf207 sf( )S407 2125 :Mf65 sf(The boundaries between the sentences that form the texts)S332 2173 :Mf181 sf<B7>Sf207 sf( )S407 2173 :Mf65 sf(The correct spelling of any words that have typographical errors in them)S332 2221 :Mf181 sf<B7>Sf207 sf( )S407 2221 :Mf65 sf(Whether words have had their natural capitalisation altered at all.)S257 2317 :M(First it is worth pointing out that the exact definition of a text token is a far from a clear matter. We)S257 2365 :M(take an agnostic view on this matter. There are several standard tokenisations that are used in English,)S257 2413 :M(such as the Penn treebank \(Marcus et al. 1993\) or the BNC tokenisation \(Burnard 1995\), use several)S257 2461 :M(different criteria. Amongst the differences between these tokenisation schemes are whether clitics are)S257 2509 :M(separated from the preceding word, whether auxiliaries with attached negative particles are detached)S257 2557 :M(\(for example is \322isn't\323 decomposed into two tokens \322is\323 and \322)S1269 2557 :M(n't\323\) and whether complex prepositions)S257 2605 :M(are considered to be single tokens or not \(as far as\). In other languages there are similar questions to be)S257 2652 :M(addressed. In German uses noun-noun compounding extensively, and such compounds are written)S257 2700 :M(without internal white space. It is perfectly legitimate to consider the segmentation of these compounds)S257 2748 :M(as part of the tokenisation process.)S257 2844 :M(We will now show a few simple examples that illustrate the range of errors that we want to account for,)S257 2892 :M(and will explain why such a seemingly simple task requires such as complex tool.)S257 2965 :Mf111 sf(Table 1 Examples of noisy text)S232 3000 66 54 rC257 3045 :Mf65 sf(1)SgRgS 0 0 2242 3254 rC323 3045 :Mf65 sf(I did with my last one.....)S232 3000 66 6 rF298 3000 6 6 rF304 3000 1705 6 rF232 3054 66 6 rF298 3054 6 6 rF304 3054 1705 6 rFendpshowpage%%PageTrailer%%Page: 3 3%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 3 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(3)SgRgS 0 0 2242 3254 rC323 216 :Mf65 sf(She is now 3 1/2 and talks )S764 216 :M(extreemly well for her age)S323 264 :M(Yet at one she signed a number of words, I think the only thing it did was)S323 312 :M(stop her getting frustrated when she wanted to communicate.. She could tell)S323 360 :M(me if she wanted a hot drink or cold drink.....if she was hungry, tired,)S323 408 :M(etc........having said that...my first 2 children didnt learn to sign BUt)S323 455 :M(could still communicate in different ways..)S232 171 66 6 rF298 171 6 6 rF304 171 1705 6 rF232 465 66 48 rC257 504 :M(2)SgRgS 0 0 2242 3254 rC323 504 :Mf65 sf(Yep. The Despots NEED Street-Level Thugs to EmPower them in their Vile)S323 552 :M(Political Manipulations.)S232 561 66 47 rC257 599 :M(3)SgRgS 0 0 2242 3254 rC323 599 :Mf65 sf(E nergy by itself has no use in respect to our )S1062 599 :M(topic)S1144 599 :M(,howi understood it.)S232 608 66 48 rC257 647 :M(4)SgRgS 0 0 2242 3254 rC323 647 :Mf65 sf(I really don't know where to start on this.  )S1017 647 :M(Dj's have been losing thier)S323 695 :M(gear to the government \(local, state, federal, whatever\) for years.)S232 704 66 48 rC257 743 :M(5)SgRgS 0 0 2242 3254 rC323 743 :Mf65 sf(For the most part, I'd say yes-they are well-pigmented.)S232 752 66 6 rF298 752 6 6 rF304 752 1705 6 rF257 844 :M(To give a concrete example of the output of the pre-processing we consider that given the second line)S257 892 :M(of the first example in Table 1, the output produced should consist at the very least of the information)S257 940 :M(that the text consists of a sentence missing a punctuation mark at the end, that the initial word \322She\323)S257 988 :M(has been capitalised since it is in sentence-initial position and that the word \322extremely\323 is a)S257 1036 :M(misspelling of \322extremely\323. In terms of the tokenisation, one could treat the string \3223 1/2\323 as 1, 2 or 4)S257 1084 :M(tokens depending on how finely one wishes to decompose the text.)S257 1180 :M(It is worth pausing now to consider why this problem is difficult. The basic problem as in so many)S257 1228 :M(problems with language is ambiguity. The orthographic features that are used in writing text are used to)S257 1275 :M(mean different things in different places. Thus the use of capitalisation is used both to mark sentence-)S257 1323 :M(initial words, and also to mark proper nouns and adjectives \(France, British\). The period is used both to)S257 1371 :M(signify the removal of material from an abbreviation, and also to mark sentence boundaries. The dash)S257 1419 :M(or minus sign \322-\323 is used both as a hyphen, as a minus sign, to signify various ranges as a parenthesis)S257 1467 :M(and so on. Some of these in theory should be represented by different characters \(-,\320,\321\) and so on, but)S257 1515 :M(in the texts we are dealing with this will not happen. When we add to this the problem of interpreting)S257 1563 :M(noise, and correcting spelling mistakes, it is clear that the problem is non-trivial. Moreover, many of)S257 1611 :M(these problems are interlinked. First, identifying sentence boundaries requires resolving the ambiguity)S257 1659 :M(of capitalisation, and of periods. Secondly, spelling errors frequently introduce errors in the)S257 1707 :M(tokenisation. A very common example in English is the incorrect use of the apostrophe before plurals,)S257 1755 :M(and the confusion between its and it's. On rare occasions we also have errors that introduce ore remove)S257 1803 :M(white space such as in Example 3 above. In addition compounds like law-abiding in English are often)S257 1850 :M(written with a hyphen or a space to mark the compound boundary. Example 5 above is a good example)S257 1898 :M(of the difficulties involved in resolving this. Moreover correcting the spelling errors requires a)S257 1946 :M(tokenisation in order to get the sentential context that can identify the words that might be misspelled.)S257 1994 :M(Thus we see that we have a tangled problem with these various processes interlocking in a rather)S257 2042 :M(complex way. We argue therefore that the appropriate solution is an integrated tool that can)S257 2090 :M(simultaneously perform the tokenisation, spelling correction, sentence boundary detection and)S257 2138 :M(capitalisation processing, and find an optimal or near-optimal solution, computing over all areas of)S257 2186 :M(ambiguity at the same time.)S257 2282 :M(There are a number of further areas of processing that also need to be done. Notably amongst this is the)S257 2330 :M(normalisation of non standard words as discussed by Sproat et al. \(2001\). Though an interesting and)S257 2377 :M(important area we feel that this can be deferred to a later stage of processing using basically a standard)S257 2425 :M(HMM tagging framework as it does not exhibit the same pernicious integration with the other)S257 2473 :M(processes. Similarly part of speech tagging and shallow parsing, and word sense disambiguation can all)S257 2521 :M(be performed later without loss of precision. An initial area of processing we have considered is also)S257 2569 :M(one specific to this corpus and to corpora of emails: namely the task of segmentation into text and non-)S257 2617 :M(text blocks which requires the identification signatures, quoting and other problem areas. We assume)S257 2665 :M(that this has been performed prior to the processing described here.)S257 2761 :Mf111 sf(Previous Approaches)Sendpshowpage%%PageTrailer%%Page: 4 4%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 4 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(4)SgRgS 0 0 2242 3254 rC257 1898 :Mf65 sf(We now describe briefly some previous approaches to this and related problems. We are not aware of)S257 1946 :M(any systems that integrate the processing to the extent we advocate here though the work of Mikheev)S257 1994 :M(\(2002\) is an interesting step in this direction. There are also standard techniques for spelling correction.)S257 2042 :M(Kernighan, Church and Gale \(1990\) is a seminal paper in this regard introducing the use of a noisy)S257 2090 :M(channel model to perform spelling correction. The idea here is to find the most likely sequence of)S257 2138 :M(words that could have given rise to the observed sequence of tokens.  This involves two components: a)S257 2186 :M(language model that will predict the sequence of correct words and an error model that models the)S257 2233 :M(conditional probability of the observed string given the correct string. This work has been extended)S257 2281 :M(since notably by Brill and Moore \(2000\) who use a more sophisticated error model to get substantial)S257 2329 :M(improvements. Note however that they are primarily concerned with errors in post-edited text and thus)S257 2377 :M(their model is designed to account for ``plausible'' errors \321i.e. errors where the author might not notice)S257 2425 :M(the error or might believe it to be correct. Limitations of this work include the fact that generally they)S257 2473 :M(do not look for errors in words that are in the dictionary. This is a serious flaw since a large amount of)S257 2521 :M(errors do in fact result in correctly spelled errors. For example, \322from\323 is often misspelt as \322form\323)S257 2569 :M(which is in the dictionary. Thus authors using simple spelling correction tools such as are often)S257 2617 :M(integrated into email clients and such like often will produce a high proportion of errors such as this)S257 2665 :M(which are overlooked by the software.)S257 2761 :M(Tokenisation of Asian languages, such as  Chinese, Japanese and Thai present particular problems for)S257 2808 :M(text processing since they are in general written without explicit indications of token boundaries in the)S257 2856 :M(form of white space. This has given rise to a series of algorithms using in general a simple Viterbi)S257 2904 :M(decoder to choose the optimal segmentation \()S1002 2904 :M(Teahan et al. 2000\). An approach that has some points of)S257 2952 :M(similarity with our own is presented in Gao et al. \(2000\). Though in English we do in general have)S257 3000 :M(white space this is not always used as is pointed out by Sproat et al \(2001\). In that paper they analyses a)S257 3048 :M(corpus of real estate advertisements which because of a cost per character are often written in a very)S1 G486 77 147 147 rF0 G3 lw484.5 75.5 150 150 rS1 G974 77 147 147 rF0 G972.5 75.5 150 150 rS310 633 150.5 @cnp 939 166 :M972 150 :L939 134 :L939 166 :Leofill1 G147 110 1047.5 619 @j0 G150 113 1047.5 619 @f1 G147 110 559.5 619 @j0 G150 113 559.5 619 @f84 559.5 224 @wnp 543 304 :M559 337 :L575 304 :L543 304 :Leofill84 1047.5 224 @wnp 1031 304 :M1047 337 :L1063 304 :L1031 304 :Leofill122 333 150.5 @cnp 451 166 :M484 150 :L451 134 :L451 166 :Leofill84 1121 150.5 @cnp 1201 166 :M1234 150 :L1201 134 :L1201 166 :Leofill1 G486 339 147 147 rF0 G484.5 337.5 150 150 rS1 G974 339 147 147 rF0 G972.5 337.5 150 150 rS1 G1272 375 599 150 rF1302 390 540 120 rC1302 436 :M0 Gf78 sf(Language Model Tokens)SgR1 GgS 0 0 2242 3254 rC1234 75 713 150 rF1264 90 654 120 rC1264 136 :M0 Gf78 sf(Language Model States)SgRgS 0 0 2242 3254 rC1272 562 600 150 rF1302 577 541 120 rC1302 623 :M0 Gf78 sf(Canonical Tokens)SgR0 GgS 0 0 2242 3254 rC3 lw47 559.5 486 @wnp 543 529 :M559 562 :L575 529 :L543 529 :Leofill47 1047.5 486 @wnp 1031 529 :M1047 562 :L1063 529 :L1031 529 :Leofill1 G486 752 147 147 rF0 G484.5 750.5 150 150 rS1 G974 752 147 147 rF0 G972.5 750.5 150 150 rS310 633 825.5 @cnp 939 841 :M972 825 :L939 809 :L939 841 :Leofill47 559.5 674 @wnp 543 717 :M559 750 :L575 717 :L543 717 :Leofill47 1047.5 674 @wnp 1031 717 :M1047 750 :L1063 717 :L1031 717 :Leofill84 371 825.5 @cnp 451 841 :M484 825 :L451 809 :L451 841 :Leofill122 1121 825.5 @cnp 1239 841 :M1272 825 :L1239 809 :L1239 841 :Leofill1 G1272 750 600 150 rF1302 765 541 120 rC1302 811 :M0 Gf78 sf(States of the Orthographic)S1302 868 :M(Transducer)SgR1 GgS 0 0 2242 3254 rC147 109 559.5 1031.5 @j0 G3 lw150 112 559.5 1031.5 @f1 G147 109 1047.5 1031.5 @j0 G150 112 1047.5 1031.5 @f47 559.5 899 @wnp 543 942 :M559 975 :L575 942 :L543 942 :Leofill47 1047.5 899 @wnp 1031 942 :M1047 975 :L1063 942 :L1031 942 :Leofill633 620 -3 3 643 618 3 633 617 @a640 621 -3 3 650 622 3 640 618 @a647 625 -3 3 657 627 3 647 622 @a654 630 -3 3 664 635 3 654 627 @a661 638 -3 3 670 644 3 661 635 @a667 647 -3 3 676 655 3 667 644 @a673 658 -3 3 682 668 3 673 655 @a679 671 -3 3 688 682 3 679 668 @a685 685 -3 3 693 697 3 685 682 @a690 700 -3 3 697 713 3 690 697 @a694 716 -3 3 705 748 3 694 713 @a702 751 -3 3 709 786 3 702 748 @a706 789 -3 3 711 824 3 706 786 @a-3 -3 710 856 3 3 708 824 @b-3 -3 707 884 3 3 707 853 @b-3 -3 703 911 3 3 704 881 @b-3 -3 697 937 3 3 700 908 @b-3 -3 690 961 3 3 694 934 @b-3 -3 682 982 3 3 687 958 @b-3 -3 673 1000 3 3 679 979 @b-3 -3 668 1008 3 3 670 997 @b-3 -3 663 1015 3 3 665 1005 @bnp 652 999 :M634 1031 :L670 1026 :L652 999 :Leofill1121 602 -3 3 1131 600 3 1121 599 @a1128 603 -3 3 1138 604 3 1128 600 @a1135 607 -3 3 1145 609 3 1135 604 @a1142 612 -3 3 1151 617 3 1142 609 @a1148 620 -3 3 1158 626 3 1148 617 @a1155 629 -3 3 1164 637 3 1155 626 @a1161 640 -3 3 1170 650 3 1161 637 @a1167 653 -3 3 1175 663 3 1167 650 @a1172 666 -3 3 1180 678 3 1172 663 @a1177 681 -3 3 1185 695 3 1177 678 @a1182 698 -3 3 1193 729 3 1182 695 @a1190 732 -3 3 1197 767 3 1190 729 @a1194 770 -3 3 1199 805 3 1194 767 @a-3 -3 1198 837 3 3 1196 805 @b-3 -3 1195 865 3 3 1195 834 @b-3 -3 1191 892 3 3 1192 862 @b-3 -3 1185 918 3 3 1188 889 @b-3 -3 1178 942 3 3 1182 915 @b-3 -3 1170 963 3 3 1175 939 @b-3 -3 1161 981 3 3 1167 960 @b-3 -3 1156 989 3 3 1158 978 @b-3 -3 1151 996 3 3 1153 986 @bnp 1140 980 :M1122 1012 :L1158 1007 :L1140 980 :Leofill1 G1309 975 601 150 rF1339 990 541 120 rC1339 1036 :M0 Gf78 sf(Correct Tokens)SgRgS 0 0 2242 3254 rC147 131 559.5 1229.5 @j0 G3 lw150 134 559.5 1229.5 @f1 G147 131 1047.5 1229.5 @j0 G150 134 1047.5 1229.5 @f47 559.5 1086 @wnp 543 1129 :M559 1162 :L575 1129 :L543 1129 :Leofill47 1047.5 1086 @wnp 1031 1129 :M1047 1162 :L1063 1129 :L1031 1129 :Leofill1 G1309 1162 601 150 rF1339 1177 541 120 rC1339 1224 :M0 Gf78 sf(Observed Tokens and)S1339 1281 :M(white space)SgRgS 0 0 2242 3254 rC147 131 822.5 1229.5 @j0 G3 lw150 134 822.5 1229.5 @f791 1194 62 71 rC791 1240 :Mf78 sf(ws)SgR0 GgS 0 0 2242 3254 rC633 620 -3 3 644 618 3 633 617 @a641 621 -3 3 653 620 3 641 618 @a650 623 -3 3 661 623 3 650 620 @a658 626 -3 3 670 628 3 658 623 @a667 631 -3 3 678 634 3 667 628 @a675 637 -3 3 686 642 3 675 634 @a683 645 -3 3 694 651 3 683 642 @a691 654 -3 3 702 660 3 691 651 @a699 663 -3 3 710 671 3 699 660 @a707 674 -3 3 718 683 3 707 671 @a715 686 -3 3 725 696 3 715 683 @a722 699 -3 3 733 710 3 722 696 @a730 713 -3 3 747 741 3 730 710 @a744 744 -3 3 761 775 3 744 741 @a758 778 -3 3 773 812 3 758 775 @a770 815 -3 3 785 852 3 770 812 @a782 855 -3 3 795 895 3 782 852 @a792 898 -3 3 804 939 3 792 895 @a801 942 -3 3 811 985 3 801 939 @a808 988 -3 3 817 1032 3 808 985 @a814 1035 -3 3 821 1080 3 814 1032 @a818 1083 -3 3 823 1129 3 818 1080 @anp 805 1130 :M822 1162 :L837 1129 :L805 1130 :L3 lweofill-3 -3 967 621 3 3 971 617 @b-3 -3 960 623 3 3 964 618 @b-3 -3 954 626 3 3 957 620 @b-3 -3 947 631 3 3 951 623 @b-3 -3 940 637 3 3 944 628 @b-3 -3 934 645 3 3 937 634 @b-3 -3 927 654 3 3 931 642 @b-3 -3 921 663 3 3 924 651 @b-3 -3 914 674 3 3 918 660 @b-3 -3 908 686 3 3 911 671 @b-3 -3 902 699 3 3 905 683 @b-3 -3 896 713 3 3 899 696 @b-3 -3 884 744 3 3 893 710 @b-3 -3 874 778 3 3 881 741 @b-3 -3 864 815 3 3 871 775 @b-3 -3 854 855 3 3 861 812 @b-3 -3 846 898 3 3 851 852 @b-3 -3 839 942 3 3 843 895 @b-3 -3 833 988 3 3 836 939 @b-3 -3 829 1035 3 3 830 985 @b-3 -3 826 1083 3 3 826 1032 @b-3 -3 824 1132 3 3 823 1080 @bnp 806 1129 :M822 1162 :L838 1129 :L806 1129 :Leofill1 G449 1411 709 147 rF0 G447.5 1409.5 712 150 rS478 1425 651 117 rC478 1471 :Mf78 sf(Raw unsegmented text)SgR1 GgS 0 0 2242 3254 rCnp 1327 1445 :M1327 1464 :L1158 1464 :L1158 1501 :L1327 1501 :L1327 1520 :L1383 1482 :L1327 1445 :L3 lweofill0 G22 1328.5 1445 @w172 1158 1465.5 @c40 1159.5 1464 @w172 1158 1502.5 @c22 1328.5 1501 @w-3 -3 1330 1523 3 3 1383 1482 @b1327 1448 -3 3 1386 1482 3 1327 1445 @a1 Gnp 333 1445 :M333 1464 :L446 1464 :L446 1501 :L333 1501 :L333 1520 :L296 1482 :L333 1445 :Leofill0 G22 334.5 1445 @w116 333 1465.5 @c40 447.5 1464 @w116 333 1502.5 @c22 334.5 1501 @w296 1485 -3 3 336 1520 3 296 1482 @a-3 -3 299 1485 3 3 333 1445 @b1 G261 -125 1722 147 rF0 G259.5 -126 1725 150 rS291 0 1662 6 rC291 0 :Mf111 sf(tokens at different levels.)SgR0 GgS 0 0 2242 3254 rC3 lw84 559.5 1295 @wnp 543 1375 :M559 1408 :L575 1375 :L543 1375 :Leofill84 822.5 1295 @wnp 806 1375 :M822 1408 :L838 1375 :L806 1375 :Leofill84 1047.5 1295 @wnp 1031 1375 :M1047 1408 :L1063 1375 :L1031 1375 :Leofillendpshowpage%%PageTrailer%%Page: 5 5%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 5 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(5)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(compact way frequently removing white space. Among other algorithms they propose an algorithm)S257 257 :M(using Finite-state automata and a Viterbi decoding algorithm. Again, there are points of similarity with)S257 305 :M(the approach presented here.)S257 401 :M(Tokenisation in English has not been extensively studied )S1200 401 :Mf241 sf(per se)Sf65 sf( largely because on clean text very)S257 448 :M(accurate results can be obtained, using basic tools such as sed or Perl or some other regular expression)S257 496 :M(based tools such as the Text Tokenisation Tool  \(Grover et al. 2000\).)S257 592 :Mf111 sf(4.)Sf147 sf( )S332 592 :Mf111 sf(Outline of approach)S257 688 :Mf65 sf(We use a straightforward machine learning methodology using generative models and a noisy channel)S257 736 :M(method. That is to say we produce a generative model for the sequence of characters in noisy text, that)S257 784 :M(is based on a sequence of tokens separated by white space.  Given a sequence of characters, we can)S257 832 :M(then use a decoding algorithm to find the most likely sequence of tokens that would have generated the)S257 880 :M(observed text; more formally, the sequence of tokens which has the highest probability given the)S257 928 :M(observed unsegmented input. This is as previously discussed rather similar to the standard noisy)S257 976 :M(channel model for spelling correction \(Kernighan, Church and Gale 1990\).)S257 1071 :M(There are a number of alternative methods that could be used -- indeed any method for chunking or)S257 1119 :M(shallow parsing could be adapted to this task, just by converting letters to words. However these)S257 1167 :M(methods would fail to take account of the multi-word context required to disambiguate the sort of)S257 1215 :M(problems displayed by the examples above.)S257 1311 :M(As previously described, our view is that the problem requires an integrated approach: namely, rather)S257 1359 :M(than treating tokenisation, sentence boundary detection and spelling correction as separate modules)S257 1407 :M(joined together in a pipeline, it is necessary for processing noisy text to integrate the two problems. Our)S257 1455 :M(approach is not here to take a system designed for clean text and gradually modify it to handle certain)S257 1503 :M(hard cases, but to design a system from the ground up to be able to process arbitrarily noisy text. In the)S257 1550 :M(next two sections we describe first the basic stochastic model that we use and its various independence)S257 1598 :M(assumptions and secondly the specific models and algorithms that we have chosen to implement.)S257 1694 :M(We now describe the model on which we base our algorithm. Effectively here we are doing language)S257 1742 :M(modelling at the character level rather than at the word level, but incorporating an n-gram word model.)S257 1790 :M(We consider the sequence of characters as consisting of a sequence of tokens interspersed by white)S257 1838 :M(space which can be of zero length. This is a rather abstract view since we allow white space to occur)S257 1886 :M(inside tokens and also allow non-white space characters to occur inside the white space. We consider)S257 1934 :M(the sequence of tokens to be generated by some sort of stochastic process that we model by a language)S257 1982 :M(model.)S257 2078 :M(A key element of our approach is that we model the process as having a sequence of levels of)S257 2125 :M(description for tokens, ranging from abstract to the observed strings, together with a set of models that)S257 2173 :M(govern the production of the more specific level from the more abstract. We will describe them briefly)S257 2221 :M(here and then present some examples that will clarify the definitions.)S332 2317 :Mf181 sf<B7>Sf207 sf( )S407 2317 :Mf65 sf(The Observed string is the actual sequence of characters that we observed in the input that)S407 2365 :M(may have been corrupted by typographical errors.)S332 2413 :Mf181 sf<B7>Sf207 sf( )S407 2413 :Mf65 sf(The Correct string is the sequence of characters that has been uncorrupted. This may not be)S407 2461 :M(correct in the sense of conforming to norms of written language.)S332 2509 :Mf181 sf<B7>Sf207 sf( )S407 2509 :Mf65 sf(The Canonical string is a representation of the string abstracted from some orthographic)S407 2557 :M(features such as capitalisation.)S257 2652 :M(Thus the simplest case is that all three levels are the same. If a word is misspelt, then the observed)S257 2700 :M(string will differ from the correct string, if the word has been capitalised or altered deliberately in some)S257 2748 :M(way, then the correct string will differ from the canonical string.)S257 2844 :Mf111 sf(Orthographic Model)S257 2940 :Mf65 sf(The canonical form of a word is just the form it is normally written in, in sentence internal position.)S257 2988 :M(The correct form is the form that it should be written in. There are two changes that can be made to the)S257 3036 :M(canonical form: first, the word can have its capitalisation altered, and secondly the trailing period can)Sendpshowpage%%PageTrailer%%Page: 6 6%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 6 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(6)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(be removed \(when the word occurs before a period or ellipsis\). The role of the orthographic model is to)S257 257 :M(handle the transduction from the canonical form to the correct form. There are a number of constraints)S257 305 :M(on how this should be done \320 it must be deterministic given both the input \(canonical\) and output)S257 353 :M(\(correct\) forms, and it should be correctly normalised. Since for technical reasons we want the program)S257 401 :M(to operate without look-ahead, we need to allow some limited non-determinism.)S257 496 :Mf111 sf(Error models)S257 592 :Mf65 sf(We next allow the process to misspell or mistype normal words. We will use this at the moment to)S257 640 :M(model two distinct processes \321 typing errors and spelling errors. These two have rather different)S257 688 :M(properties. True typing errors are caused when the typist intends to produce a particular sequence of)S257 736 :M(key-strokes and fails; spelling errors are caused when someone genuinely believes that a word is spelt)S257 784 :M(in a particular incorrect way. They differ in two respects: first typing errors generally consist of)S257 832 :M(metathesis \(the transposition of two adjacent letters\), and substitution errors where an adjacent key is)S257 880 :M(substituted for the correct, such as for example \324f\325 being substituted for \324d\325. Spelling errors, on the other)S257 928 :M(hand, often involve phonological proximity particularly with vowels being substituted for each other.)S257 976 :M(The second difference is that typing errors are more independent stochastically. Spelling errors will)S257 1023 :M(often be repeated when the word concerned re-appears in a text written by the same author. In post-)S257 1071 :M(edited texts, we need also to distinguish a third process: accidental typographical errors that are not)S257 1119 :M(corrected during the editing process. In this case we will have a subset of typing errors that are)S257 1167 :M(superficially plausible. Glaring errors such as an insertion of an f before a d will be corrected, and more)S257 1215 :M(subtle errors will remain.)S257 1311 :Mf111 sf(White space type)S257 1407 :Mf65 sf(We condition the white space model on the preceding and following token.  This seems to be the)S257 1455 :M(minimum necessary. The fact that periods and other punctuation marks are written immediately)S257 1503 :M(following the previous token, and the fact that currency symbols are written \(generally\) immediately)S257 1550 :M(preceding the quantities they modify seems to justify this. We are not aware of any longer distance)S257 1598 :M(effects where white space is a factor. Note also that white space following a sentence boundary often)S257 1646 :M(has one or more new lines, and that some punctuation marks are often followed by more than one)S257 1694 :M(space.)S257 1790 :Mf111 sf(Discussion of overall model)S257 1886 :Mf65 sf(The diagram in Figure 1 shows the overall structure of the model.  The complexity of the model allows)S257 1934 :M(a certain amount of flexibility in how we treat particular phenomena. So for example some people do)S257 1982 :M(not use any capitalisation at all. This could be modelled either by the orthographic model, or by the)S257 2030 :M(error model. To model it using the orthographic model we add a state or states that represent the)S257 2078 :M(removal of all capitalisation, and add some transitions, and the appropriate transductions. Modelling it)S257 2125 :M(using the error model would involve having either a higher probability for substitution errors such as)S257 2173 :M(\322A\323 being replaced by \322a\323, or modelling the errors as a composition of two transducers, one which)S257 2221 :M(models the changes in case, and the other which models the errors which change the characters further.)S257 2269 :M(Clearly the independence assumptions in this case favour the former approach, and accordingly we)S257 2317 :M(have used this.)S257 2413 :M(There are however a number of situations where the model is insufficiently general. First, there are)S257 2461 :M(limitations to do with the decoding algorithm: in particular the model cannot correct words that are not)S257 2509 :M(in the word list of the fast match. Thus if one encountered a new word that ended in \322)S1659 2509 :M(ollogy\323, even)S257 2557 :M(without having encountered the corresponding word ending in \322)S1307 2557 :M(ology\323, orthotactic constraints would)S257 2605 :M(indicate that it is misspelled. It is technically rather difficult to model this process, and it is not clear)S257 2652 :M(how important it is in practice. Secondly, there are a broad range of errors where the errors are not)S257 2700 :M(limited to an individual token that cannot be processed by this model. In particular, typographical)S257 2748 :M(errors that span more than one token cannot be handled correctly. e.g. isn't misspelt as ins't, a fairly)S257 2796 :M(common error that occurs about 50 times in our corpus. This can be handled as two separate)S257 2844 :M(substitution errors, but not as the transposition. Next, there are some errors which involve the sequence)S257 2892 :M(of tokens themselves, such as tokens that are duplicated or omitted. Examples such as the following can)S257 2940 :M(be found easily)S407 2988 :M(Also  I found the the ground here)S407 3036 :M(this year does not seem to hold water?)Sendpshowpage%%PageTrailer%%Page: 7 7%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 7 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(7)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(They could perhaps better be handled by a tagging/shallow parser component, just as the use of)S257 257 :M(repetition deliberately \322very, very, very worrying\323 is handled, or ellipis in general. Tmesis is also a)S257 305 :M(phenomenon which is difficult to handle with this model.)S407 353 :M(Fan-Fucking-Tastic. I know this film has its detractors. Too many people)S407 401 :M(expecting another "Martin" or "Dawn Of The Dead".)S257 496 :M(Spelling mistakes inside composite tokens are also not treated properly.  If the compound token is in)S257 544 :M(the fast match then it will be handled properly, otherwise we would need a separate component for)S257 592 :M(dealing with this. Since such a component is probably necessary anyway, we do not feel this is a)S257 640 :M(serious problem, as it does not interfere with the tokenisation.)S257 736 :Mf111 sf(5.)Sf147 sf( )S332 736 :Mf111 sf(Specific Algorithms)S257 832 :Mf65 sf(We now can describe the algorithm based on this model more specifically. Our decision to treat all of)S257 880 :M(these problems at the same time obviously leads to substantially increased complexity. We have)S257 928 :M(therefore taken care to decompose the tool into various modules, with cleanly defined interfaces that)S257 976 :M(can be altered separately without causing problems. The main modules of the system thus correspond)S257 1023 :M(basically to the various statistical models we have used and are as described in the following sections.)S257 1071 :M(This modularity has some negative aspects. First, there is a certain amount of overlap in the lexical)S257 1119 :M(resources used in identifying the classes.  So for example clearly the type of a token as defined by the)S257 1167 :M(white space model and the type used by the orthographic model may have substantial overlap. This)S257 1215 :M(causes inefficiencies both in terms of time \(the same classification may be performed by more than one)S257 1263 :M(component\) and in terms of space \(since word lists and so on may be loaded by more than one)S257 1311 :M(component\).)S257 1407 :M(The language model we use is a trigram language model with interpolated Kneser Ney smoothing)S1860 1407 :M(.\()S1884 1407 :M(Ney)S257 1455 :M(et al. 1994\). This is a deterministic model with the states of the language model corresponding to)S257 1503 :M(bigrams of the language model tokens. Any other deterministic language model could also be used.)S257 1550 :M(Rather than having an out of vocabulary \(OOV\) token that we ignore, we have what is sometimes)S257 1598 :M(called a hierarchical language model \(Galescu and Allen 2000\) that incorporates separate models for)S257 1646 :M(the sequences of characters that make up unknown words. We use several different classes of OOV)S257 1694 :M(token here that can be used to model the unknown words more precisely.  We will not give an)S257 1742 :M(exhaustive list of the classes here since to certain extent this is application dependent. We will merely)S257 1790 :M(outline the circumstances under which it might be desirable to have a separate type. The first types are)S257 1838 :M(of course just basic words which have a single invariant form. The vast majority of forms are like that.)S257 1886 :M(Next we have true out-of-vocabulary tokens representing words that we have not seen yet. In practice)S257 1934 :M(since we want to model the co-occurrence of these tokens with tokens we have already seen, we will)S257 1982 :M(also include a section of words that we have seen already but that are very rare. We will divide these)S257 2030 :M(into classes: at the very least separate classes for numbers and normal words, but optionally also)S257 2078 :M(separate classes for different part of speech tags for example. Technically we would wish the sets of)S257 2125 :M(strings in each class to be disjoint but this is not vital -- the only result would be a slightly deficient)S257 2173 :M(model. In addition there are two other situations in which it might be desirable to model the token as a)S257 2221 :M(set of strings. First we have variant spellings of particular words, whether due to dialect variations)S257 2269 :M(\(colour/)S387 2269 :M(color\) or register variations \(tonight/)S986 2269 :M(tonite\).  Modelling these alternations reduces the amount)S257 2317 :M(of data required.  Secondly we have tokens which have a variable length. The following tables will)S257 2365 :M(illustrate some of these. First, the use of exclamation marks and question marks is rather free, as can be)S257 2413 :M(seen in this table which shows the frequency of occurrence of strings of up to 10 symbols.)S257 2486 :Mf111 sf(Table 1)S388 2486 :M(: Productive use of punctuation marks.)S257 2562 :Mf65 sf(80724)S505 2562 :M(!)S805 2562 :M(220501)S1105 2562 :M(?)S233 2521 2 2 rF233 2521 2 2 rF235 2521 246 2 rF481 2521 2 2 rF483 2521 298 2 rF781 2521 2 2 rF783 2521 298 2 rF1081 2521 2 2 rF1083 2521 336 2 rF1419 2521 2 2 rF1419 2521 2 2 rF233 2523 2 48 rF481 2523 2 48 rF781 2523 2 48 rF1081 2523 2 48 rF1419 2523 2 48 rF257 2612 :M(3682)S505 2612 :M(!!)S805 2612 :M(3675)S1105 2612 :M(??)S233 2571 2 2 rF235 2571 246 2 rF481 2571 2 2 rF483 2571 298 2 rF781 2571 2 2 rF783 2571 298 2 rF1081 2571 2 2 rF1083 2571 336 2 rF1419 2571 2 2 rF233 2573 2 48 rF481 2573 2 48 rF781 2573 2 48 rF1081 2573 2 48 rF1419 2573 2 48 rF257 2662 :M(3719)S505 2662 :M(!!!)S805 2662 :M(2857)S1105 2662 :M(???)S233 2621 2 2 rF235 2621 246 2 rF481 2621 2 2 rF483 2621 298 2 rF781 2621 2 2 rF783 2621 298 2 rF1081 2621 2 2 rF1083 2621 336 2 rF1419 2621 2 2 rF233 2623 2 48 rF481 2623 2 48 rF781 2623 2 48 rF1081 2623 2 48 rF1419 2623 2 48 rF257 2712 :M(1044)S505 2712 :M(!!!!)S805 2712 :M(738)S1105 2712 :M(????)S233 2671 2 2 rF235 2671 246 2 rF481 2671 2 2 rF483 2671 298 2 rF781 2671 2 2 rF783 2671 298 2 rF1081 2671 2 2 rF1083 2671 336 2 rF1419 2671 2 2 rF233 2673 2 48 rF481 2673 2 48 rF781 2673 2 48 rF1081 2673 2 48 rF1419 2673 2 48 rF257 2762 :M(639)S505 2762 :M(!!!!!)S805 2762 :M(291)S1105 2762 :M(?????)S233 2721 2 2 rF235 2721 246 2 rF481 2721 2 2 rF483 2721 298 2 rF781 2721 2 2 rF783 2721 298 2 rF1081 2721 2 2 rF1083 2721 336 2 rF1419 2721 2 2 rF233 2723 2 48 rF481 2723 2 48 rF781 2723 2 48 rF1081 2723 2 48 rF1419 2723 2 48 rF257 2812 :M(262)S505 2812 :M(!!!!!!)S805 2812 :M(113)S1105 2812 :M(??????)S233 2771 2 2 rF235 2771 246 2 rF481 2771 2 2 rF483 2771 298 2 rF781 2771 2 2 rF783 2771 298 2 rF1081 2771 2 2 rF1083 2771 336 2 rF1419 2771 2 2 rF233 2773 2 48 rF481 2773 2 48 rF781 2773 2 48 rF1081 2773 2 48 rF1419 2773 2 48 rF257 2862 :M(202)S505 2862 :M(!!!!!!!)S805 2862 :M(73)S1105 2862 :M(???????)S233 2821 2 2 rF235 2821 246 2 rF481 2821 2 2 rF483 2821 298 2 rF781 2821 2 2 rF783 2821 298 2 rF1081 2821 2 2 rF1083 2821 336 2 rF1419 2821 2 2 rF233 2823 2 48 rF481 2823 2 48 rF781 2823 2 48 rF1081 2823 2 48 rF1419 2823 2 48 rF257 2912 :M(106)S505 2912 :M(!!!!!!!!)S805 2912 :M(79)S1105 2912 :M(????????)S233 2871 2 2 rF235 2871 246 2 rF481 2871 2 2 rF483 2871 298 2 rF781 2871 2 2 rF783 2871 298 2 rF1081 2871 2 2 rF1083 2871 336 2 rF1419 2871 2 2 rF233 2873 2 48 rF481 2873 2 48 rF781 2873 2 48 rF1081 2873 2 48 rF1419 2873 2 48 rF257 2962 :M(101)S505 2962 :M(!!!!!!!!!)S805 2962 :M(15)S1105 2962 :M(?????????)S233 2921 2 2 rF235 2921 246 2 rF481 2921 2 2 rF483 2921 298 2 rF781 2921 2 2 rF783 2921 298 2 rF1081 2921 2 2 rF1083 2921 336 2 rF1419 2921 2 2 rF233 2923 2 48 rF481 2923 2 48 rF781 2923 2 48 rF1081 2923 2 48 rF1419 2923 2 48 rF257 3012 :M(70)S505 3012 :M(!!!!!!!!!!)S805 3012 :M(8)S1105 3012 :M(??????????)S233 2971 2 2 rF235 2971 246 2 rF481 2971 2 2 rF483 2971 298 2 rF781 2971 2 2 rF783 2971 298 2 rF1081 2971 2 2 rF1083 2971 336 2 rF1419 2971 2 2 rF233 2973 2 48 rF233 3021 2 2 rF233 3021 2 2 rF235 3021 246 2 rF481 2973 2 48 rF481 3021 2 2 rF483 3021 298 2 rF781 2973 2 48 rF781 3021 2 2 rF783 3021 298 2 rF1081 2973 2 48 rF1081 3021 2 2 rF1083 3021 336 2 rF1419 2973 2 48 rF1419 3021 2 2 rF1419 3021 2 2 rFendpshowpage%%PageTrailer%%Page: 8 8%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 8 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(8)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(The next table illustrates the use of another word \322no\323 in our corpus, together with a comparison with)S257 257 :M(the frequency of occurrence in a million words of the Penn Treebank. As well as illustrating the)S257 305 :M(necessity of a productive model for this word, it highlights the vast difference in the number of)S257 353 :M(examples one can get from this sort of noisy corpus, compared to a small clean corpus, when it comes)S257 401 :M(to an extremely common word such as \322no\323.)S257 473 :Mf111 sf(Table 2)S388 473 :M(: Comparison of counts of variants of "no" in Penn Treebank and the Usenet corpus.)S257 549 :Mf65 sf(String)S849 549 :M(Penn Treebank count)S1442 549 :M(Usenet count)S233 508 2 2 rF233 508 2 2 rF235 508 591 2 rF826 508 2 2 rF828 508 590 2 rF1418 508 2 2 rF1420 508 590 2 rF2010 508 2 2 rF2010 508 2 2 rF233 510 2 48 rF826 510 2 48 rF1418 510 2 48 rF2010 510 2 48 rF257 599 :M(no)S849 599 :M(936)S1442 599 :M(404114)S233 558 2 2 rF235 558 591 2 rF826 558 2 2 rF828 558 590 2 rF1418 558 2 2 rF1420 558 590 2 rF2010 558 2 2 rF233 560 2 48 rF826 560 2 48 rF1418 560 2 48 rF2010 560 2 48 rF257 649 :M(No)S849 649 :M(111)S1442 649 :M(45429)S233 608 2 2 rF235 608 591 2 rF826 608 2 2 rF828 608 590 2 rF1418 608 2 2 rF1420 608 590 2 rF2010 608 2 2 rF233 610 2 48 rF826 610 2 48 rF1418 610 2 48 rF2010 610 2 48 rF257 699 :M(NO)S849 699 :M(2)S1442 699 :M(12831)S233 658 2 2 rF235 658 591 2 rF826 658 2 2 rF828 658 590 2 rF1418 658 2 2 rF1420 658 590 2 rF2010 658 2 2 rF233 660 2 48 rF826 660 2 48 rF1418 660 2 48 rF2010 660 2 48 rF257 749 :M(noo)S849 749 :M(0)S1442 749 :M(26)S233 708 2 2 rF235 708 591 2 rF826 708 2 2 rF828 708 590 2 rF1418 708 2 2 rF1420 708 590 2 rF2010 708 2 2 rF233 710 2 48 rF826 710 2 48 rF1418 710 2 48 rF2010 710 2 48 rF257 799 :M(Noo)S849 799 :M(0)S1442 799 :M(31)S233 758 2 2 rF235 758 591 2 rF826 758 2 2 rF828 758 590 2 rF1418 758 2 2 rF1420 758 590 2 rF2010 758 2 2 rF233 760 2 48 rF826 760 2 48 rF1418 760 2 48 rF2010 760 2 48 rF257 849 :M(NOO)S849 849 :M(0)S1442 849 :M(1)S233 808 2 2 rF235 808 591 2 rF826 808 2 2 rF828 808 590 2 rF1418 808 2 2 rF1420 808 590 2 rF2010 808 2 2 rF233 810 2 48 rF826 810 2 48 rF1418 810 2 48 rF2010 810 2 48 rF257 899 :M(nooo)S849 899 :M(0)S1442 899 :M(7)S233 858 2 2 rF235 858 591 2 rF826 858 2 2 rF828 858 590 2 rF1418 858 2 2 rF1420 858 590 2 rF2010 858 2 2 rF233 860 2 48 rF826 860 2 48 rF1418 860 2 48 rF2010 860 2 48 rF257 949 :M(Nooo)S849 949 :M(0)S1442 949 :M(2)S233 908 2 2 rF235 908 591 2 rF826 908 2 2 rF828 908 590 2 rF1418 908 2 2 rF1420 908 590 2 rF2010 908 2 2 rF233 910 2 48 rF826 910 2 48 rF1418 910 2 48 rF2010 910 2 48 rF257 999 :M(NOO)S849 999 :M(0)S1442 999 :M(4)S233 958 2 2 rF235 958 591 2 rF826 958 2 2 rF828 958 590 2 rF1418 958 2 2 rF1420 958 590 2 rF2010 958 2 2 rF233 960 2 48 rF233 1008 2 2 rF233 1008 2 2 rF235 1008 591 2 rF826 960 2 48 rF826 1008 2 2 rF828 1008 590 2 rF1418 960 2 48 rF1418 1008 2 2 rF1420 1008 590 2 rF2010 960 2 48 rF2010 1008 2 2 rF2010 1008 2 2 rF257 1096 :M(Similarly the following table shows a small subset of the ways in which authors express laughter and)S257 1144 :M(surprise. This class has a large number of different types attested, and is a very good argument for)S257 1192 :M(showing that we need a model at the word list more sophisticated than a simple list of possibilities.)S257 1265 :Mf111 sf(Table 3)S388 1265 :M(: A productive word class.)S257 1341 :Mf65 sf(ha)S655 1341 :M(7459)S233 1300 2 2 rF233 1300 2 2 rF235 1300 396 2 rF631 1300 2 2 rF633 1300 298 2 rF931 1300 2 2 rF931 1300 2 2 rF233 1302 2 48 rF631 1302 2 48 rF931 1302 2 48 rF257 1391 :M(Ha)S655 1391 :M(2617)S233 1350 2 2 rF235 1350 396 2 rF631 1350 2 2 rF633 1350 298 2 rF931 1350 2 2 rF233 1352 2 48 rF631 1352 2 48 rF931 1352 2 48 rF257 1441 :M(HA)S655 1441 :M(1887)S233 1400 2 2 rF235 1400 396 2 rF631 1400 2 2 rF633 1400 298 2 rF931 1400 2 2 rF233 1402 2 48 rF631 1402 2 48 rF931 1402 2 48 rF257 1491 :M(haha)S655 1491 :M(664)S233 1450 2 2 rF235 1450 396 2 rF631 1450 2 2 rF633 1450 298 2 rF931 1450 2 2 rF233 1452 2 48 rF631 1452 2 48 rF931 1452 2 48 rF257 1541 :M(hahaha)S655 1541 :M(249)S233 1500 2 2 rF235 1500 396 2 rF631 1500 2 2 rF633 1500 298 2 rF931 1500 2 2 rF233 1502 2 48 rF631 1502 2 48 rF931 1502 2 48 rF257 1591 :M(hah)S655 1591 :M(214)S233 1550 2 2 rF235 1550 396 2 rF631 1550 2 2 rF633 1550 298 2 rF931 1550 2 2 rF233 1552 2 48 rF631 1552 2 48 rF931 1552 2 48 rF257 1641 :M(ha-ha)S655 1641 :M(213)S233 1600 2 2 rF235 1600 396 2 rF631 1600 2 2 rF633 1600 298 2 rF931 1600 2 2 rF233 1602 2 48 rF631 1602 2 48 rF931 1602 2 48 rF257 1691 :M(Hah)S655 1691 :M(213)S233 1650 2 2 rF235 1650 396 2 rF631 1650 2 2 rF633 1650 298 2 rF931 1650 2 2 rF233 1652 2 48 rF631 1652 2 48 rF931 1652 2 48 rF257 1741 :M(Hahaha)S655 1741 :M(162)S233 1700 2 2 rF235 1700 396 2 rF631 1700 2 2 rF633 1700 298 2 rF931 1700 2 2 rF233 1702 2 48 rF631 1702 2 48 rF931 1702 2 48 rF257 1791 :M(hahahahaha)S655 1791 :M(152)S233 1750 2 2 rF235 1750 396 2 rF631 1750 2 2 rF633 1750 298 2 rF931 1750 2 2 rF233 1752 2 48 rF233 1800 2 2 rF233 1800 2 2 rF235 1800 396 2 rF631 1752 2 48 rF631 1800 2 2 rF633 1800 298 2 rF931 1752 2 48 rF931 1800 2 2 rF931 1800 2 2 rF257 1888 :Mf111 sf(Fast Match)S257 1984 :Mf65 sf(The Fast Match is the component that we use to identify possible correct spellings of observed words.)S257 2032 :M(Given an observed string in the input, we normalise it by removing non-alphabetic characters and then)S257 2080 :M(retrieve from the fast match all words that are sufficiently close in terms of Levenshtein edit distance.)S257 2128 :M(In general, the word list of the fast match should be much larger than the vocabulary of the Language)S257 2175 :M(Model. We have experimented with various algorithms for this including Burkhard-Keller trees but)S257 2223 :M(have obtained best results by using a Prefix Tree Acceptor built on the reversed strings, where it is easy)S257 2271 :M(to compute the Levenshtein distance for all suffixes of a given string with all the words in the word list)S257 2319 :M(very efficiently.)S257 2415 :Mf111 sf(Orthographic Transducer)S257 2511 :Mf65 sf(The Orthographic model is responsible for the mapping between the canonical forms and the correct)S257 2559 :M(forms. There are basically two sorts of changes that are made. We model the mapping between)S257 2607 :M(canonical forms and correct forms as a fully-aligned non-deterministic stochastic finite-state)S257 2655 :M(transducer. There are slight problems with the normalisation of this model. There are basically two)S257 2702 :M(independent processes that need to be modelled \321 one which is the use of capitalisation, that is)S257 2750 :M(conversion of upper and lower cases, and the other which is the removal of trailing periods from)S257 2798 :M(abbreviations before periods and ellipses. Neither of these processes is performed consistently, and thus)S257 2846 :M(we must model them as a stochastic process. We model them as two independent transducers and use)S257 2894 :M(the product of them which reduces the number of parameters and the complexity considerably. To give)S257 2942 :M(a concrete example the transducer models the transduction from the string of canonical tokens \322perhaps)S257 2990 :M(, but even if I did, that in no way means\323 to the observed text which is \322Perhaps, but even if I did, that)S257 3038 :M(in NO WAY means\323. Given a particular input string of tokens, there are many possible output)Sendpshowpage%%PageTrailer%%Page: 9 9%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 9 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1113 3174 22 48 rC1113 3212 :Mf65 sf(9)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(sequences, since emphasis can be applied in many different places. A problem with this mode of)S257 257 :M(analysis is that because we assume that we lose the information that particular phrases are more often)S257 305 :M(emphasized than others.  Of course, if this turned out to be a problem, in practice we could add the)S257 353 :M(emphasized tokens to the language model.)S257 448 :Mf111 sf(White Space Models)S257 544 :Mf65 sf(The white space model is not very critical. There are basically three or four different circumstances we)S257 592 :M(want to distinguish. First, we have the normal situation where we have the white space between two)S257 640 :M(normal words which is normally represented by a single white space character or a new-line character)S257 688 :M(if it is at the end of a line. Secondly, we have the space between a word and a clitic which is invariably)S257 736 :M(of length zero. Thirdly we have the space between currency symbols and the numbers they modify,)S257 784 :M(which is normally zero but frequently has spaces. Finally, we have various classes that model the white)S257 832 :M(space before and after and between various different classes of punctuation symbols. The specific)S257 880 :M(models we use are character trigram models with boundary symbols, which seem to have enough)S257 928 :M(structure to capture the limited length dependencies.)S257 1023 :Mf111 sf(Error Model)S257 1119 :Mf65 sf(We use trainable stateless stochastic transducers to model typographical errors. We bootstrapped some)S257 1167 :M(models using the data presented in Kernighan, Church and Gale \(1990\), but we found the distribution)S257 1215 :M(of errors in our data to be radically different from the results they report. We intend to experiment with)S257 1263 :M(more sophisticated error models, but at the moment we feel the additional computation cannot be)S257 1311 :M(justified. We focus rather on modelling the particular classes of errors that are critical to tokenisation,)S257 1359 :M(such as incorrect use of apostrophes.)S257 1455 :Mf111 sf(Viterbi Decoder)S257 1550 :Mf65 sf(The decoder is the component which determines the most likely sequence of language model tokens)S257 1598 :M(given just the observed strings. We use a Viterbi decoder with a beam search. For each character in the)S257 1646 :M(input we store the hypotheses that terminate at that point together with the log probabilities. We only)S257 1694 :M(store the best hypothesis that ends in a given language model state and state of the orthographic)S257 1742 :M(transducer.  In addition in order to reduce the search space we only consider further those hypotheses)S257 1790 :M(that are sufficiently close to the best hypothesis. This can take the form either of considering only the)S257 1838 :M(n-best hypotheses, or considering all those hypotheses whose log probabilities lie within a certain)S257 1886 :M(amount of the best hypothesis. This is necessary since otherwise we would have to consider a number)S257 1934 :M(of hypotheses at worst equal to the square of the vocabulary size. In addition we use some )S1744 1934 :Mf241 sf(ad hoc)S257 1982 :Mf65 sf(heuristics to prune the set of tokens we consider at each character, based on the unigram probability)S257 2030 :M(and the error model probability. The precise form of the algorithm is rather complicated since we need)S257 2078 :M(to operate the orthographic model in both directions, producing candidate tokens for words in the fast)S257 2125 :M(match, and also reversing the process and reconstructing the canonical form for new words. We shall)S257 2173 :M(not describe it here.)S257 2269 :Mf111 sf(6.)Sf147 sf( )S332 2269 :Mf111 sf(Discussion)S257 2365 :Mf65 sf(We have presented a tool for performing a closely related set of low-level processing tasks on)S257 2413 :M(arbitrarily noisy text. We argue that the intertwining of these different models makes such an integrated)S257 2461 :M(tool necessary.  Part of the motivation for this work has been to acquire lexical resources from less)S257 2509 :M(formal registers of English that could then be used in further processing and in providing more)S257 2557 :M(appropriate language models for conversational speech that newspaper or newswire derived ones.  A)S257 2605 :M(second motivation is to provide very robust front end for written natural language dialogue;)S257 2652 :M(accordingly we have implemented this tool in such a way that it can run asynchronously with the input)S257 2700 :M(stream in a multi-threaded environment. At the time of writing the tool is fully implemented in Java,)S257 2748 :M(and we are engaged in tuning the models, and performing various boot-strapping operations. The)S257 2796 :M(modularity of the system allows us to run it in various different modes, for example assuming that the)S257 2844 :M(text has no spelling errors, or has all tokens separated by white space. We are using the Penn)S257 2892 :M(tokenisation on a large corpus of English Usenet posts.)S257 2988 :M(In terms of future work, our most pressing concern is to perform a proper evaluation.  Since it is very)S257 3036 :M(time consuming to manually annotate data with tokenisations, currently we use a crude )S1694 3036 :M(tokeniser to)Sendpshowpage%%PageTrailer%%Page: 10 10%%BeginPageSetup%RBIIncludePageSlotInvocationmTSsetuppmSVsetupinitializepage(Clare Hornsby; page: 10 of 10)setjob%%EndPageSetupgS 0 0 2242 3254 rC1102 3174 43 48 rC1102 3212 :Mf65 sf(10)SgRgS 0 0 2242 3254 rC257 209 :Mf65 sf(produce a first draft that we then manually correct. This will obviously tend to under-report errors)S257 257 :M(significantly. A further problem is that the gold standard is not always perfectly clear. Frequently there)S257 305 :M(can be different possible analyses that are fairly plausible. A check on inter-annotator agreement seems)S257 353 :M(to be necessary. Additionally, the data we use is very mixed. Much of our Usenet corpus is very clean)S257 401 :M(and highly edited. Frequently people post excerpts from press releases or copyrighted news material.)S257 448 :M(We are mostly interested in the noisiest segments of this data, where the additional complexity of our)S257 496 :M(approach is justified.)S257 592 :M(Currently we use single models for many of the components. An improvement might be to use mixture)S257 640 :M(models which could track different styles of typographical errors. A more challenging problem is to)S257 688 :M(separate intentional errors due probably to ignorance of the correct spelling, from accidental)S257 736 :M(typographical errors, for improved batch processing.)S257 832 :M(There are a number of other areas in which people use rather non-standard orthography. The use of)S257 880 :M(asterisks or underscores as emphasis \322_really_\323 or \322*very*\323 is quite common. Sentence boundaries are)S257 928 :M(not always marked with a specific punctuation mark, but sometimes just by white space \(in particular)S257 976 :M(new lines\). Line length in this case is an important clue.)S257 1071 :Mf111 sf(Acknowledgments)S257 1167 :Mf65 sf(This work has been supported by the Multimodal Dialogue Processing IP \(issco-)S257 1215 :M(www.unige.ch/projects/im2/mdm\) of the IM2 project \(www.im2.ch\).)S257 1311 :Mf111 sf(References)S257 1407 :Mf65 sf(Brill E, Moore, R C. 2000 An improved error model for noisy channel spelling correction. )Sf241 sf(Proceedings)S257 1455 :M(of ACL 2000)S467 1455 :Mf65 sf(.)S257 1503 :M(Burnard L 1995 )Sf241 sf(Users Reference Guide for the British National Corpus)Sf65 sf(.)S257 1550 :M(Galescu L, Allen, J.2000 Evaluating hierarchical hybrid language models. )S1481 1550 :Mf241 sf(Proceedings of ICSLP 2000)S1940 1550 :Mf65 sf(,)S257 1598 :M(Beijing, China, October.)S257 1646 :M(Grover C, Matheson C, Mikheev A, Moens)S969 1646 :M( M 2000 .LT TTT \321 a flexible tokenisation tool.)S257 1694 :Mf241 sf(Proceedings of LREC 2000)S707 1694 :Mf65 sf(.)S257 1742 :M(Gao J, Wang H, Li M, Lee K-F 2000 A unified approach to statistical language modelling for Chinese.)S257 1790 :Mf241 sf(Proceedings of ICASSP)Sf65 sf(, Istanbul, Turkey,.)S257 1838 :M(Jelinek F.1997 )S509 1838 :Mf241 sf(Statistical methods for speech recognition)Sf65 sf( MIT Press.)S257 1886 :M(Kernighan M)S477 1886 :M(,  Church K, Gale W 1990 A spelling correction program based on a noisy channel model.)S257 1934 :Mf241 sf(Proceedings of COLING 1990)S757 1934 :Mf65 sf(, pp 205-210.)S257 1982 :M(Mikheev A 2002. Periods, capitalized words, etc. Computational Linguistics, 28\(3\):289\320318.)S257 2030 :M(Marcus M, Santorini B, Marcinkiewicz M 1993 Building a large annotated corpus of English: the Penn)S257 2078 :M(Treebank )Sf241 sf(Computational Linguistics)Sf65 sf(, 19.)S257 2125 :M(Ney H, Essen U, Kneser R 1994. On structuring probabilistic dependencies in stochastic language)S257 2173 :M(modelling. )S444 2173 :Mf241 sf(Computer Speech and Language)Sf65 sf(, 8:1\32038.)S257 2221 :M(Paul D, Baker J 1997. The design for the Wall Street Journal-based CSR corpus. )S1588 2221 :M(In )Sf241 sf(Proceedings of the)S257 2269 :M(DARPA SLS Workshop)Sf65 sf(, February.)S257 2317 :M(Sproat R, Black A, Chen S, Kumar S, )S886 2317 :M(Ostendorf M, Richards C.2001Normalization of non-standard)S257 2365 :M(words. )S378 2365 :Mf241 sf(Computer Speech and Language)Sf65 sf(, 15\(3\):287\320333.)S257 2413 :M(Teahan W, Wen Y, )S585 2413 :M(McNab R, Witten I 2000 A compression-based algorithm for Chinese word)S257 2461 :M(segmentation. )Sf241 sf(Computational Linguistics)Sf65 sf(, 26\(3\):375\320393.)Sendpshowpage%%PageTrailer%%Trailerend%%EOF